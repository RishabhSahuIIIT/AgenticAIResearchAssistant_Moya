{
  "filename": "3544902.3546238.pdf",
  "text": "On the Relationship Between Story Points and Development\nEffort in Agile Open-Source Software\nVali Tawosi\nUniversity College London\nLondon, UK\nvali.tawosi@ucl.ac.uk\nRebecca Moussa\nUniversity College London\nLondon, UK\nrebecca.moussa.18@ucl.ac.uk\nFederica Sarro\nUniversity College London\nLondon, UK\nf.sarro@ucl.ac.uk\nABSTRACT\nBackground: Previous work has provided some initial evidence\nthat Story Point (SP) estimated by human-experts may not accu-\nrately reflect the effort needed to realise Agile software projects.\nAims: In this paper, we aim to shed further light on the relationship\nbetween SP and Agile software development effort to understand\nthe extent to which human-estimated SP is a good indicator of\nuser story development effort expressed in terms of time needed to\nrealise it. Method: To this end, we carry out a thorough empirical\nstudy involving a total of 37,440 unique user stories from 37 differ-\nent open-source projects publicly available in the TAWOS dataset.\nFor these user stories, we investigate the correlation between the\nissue development time (or its approximation when the actual time\nis not available) and the SP estimated by human-expert by using\nthree widely-used correlation statistics (i.e., Pearson, Kendall and\nSpearman). Furthermore, we investigate SP estimations made by\nthe human-experts in order to assess the extent to which they are\nconsistent in their estimations throughout the project, i.e., we as-\nsess whether the development time of the issues is proportionate\nto the SP assigned to them. Results: The average results across\nthe three correlation measures reveal that the correlation between\nthe human-expert estimated SP and the approximated develop-\nment time is strong for only 7% of the projects investigated, and\nmedium (58%) or low (35%) for the remaining ones. Similar results\nare obtained when the actual development time is considered. Our\nempirical study also reveals that the estimation made is often not\nconsistent throughout the project and the human estimator tends\nto misestimate in 78% of the cases. Conclusions: Our empirical\nresults suggest that SP might not be an accurate indicator of open-\nsource Agile software development effort expressed in terms of\ndevelopment time. The impact of its use as an indicator of effort\nshould be explored in future work, for example as a cost-driver in\nautomated effort estimation models or as the prediction target.\nCCS CONCEPTS\n• Software and its engineering;\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nESEM ’22, September 19–23, 2022, Helsinki, Finland\n© 2022 Association for Computing Machinery.\nACM ISBN 978-1-4503-9427-7/22/09...$15.00\nhttps://doi.org/10.1145/3544902.3546238\nKEYWORDS\nSoftware effort estimation; Story Point; Agile software.\nACM Reference Format:\nVali Tawosi, Rebecca Moussa, and Federica Sarro. 2022. On the Rela-\ntionship Between Story Points and Development Effort in Agile Open-\nSource Software. In ACM / IEEE International Symposium on Empirical\nSoftware Engineering and Measurement (ESEM) (ESEM ’22), September 19–\n23, 2022, Helsinki, Finland. ACM, New York, NY, USA, 12 pages. https:\n//doi.org/10.1145/3544902.3546238\n1\nINTRODUCTION\nSoftware Effort Estimation (SEE) is a crucial activity for managing,\nplanning, and monitoring software projects [53]. Without an accu-\nrate estimation of the effort required to develop software, budget\nand schedule overrun seem inevitable [21, 44]. SEE research has\nmainly focused on estimating the effort required to develop a whole\nproject (i.e., project-level estimation). To this end, Functional Size\nMeasures (FSM), such as Function Point (FP) [6] or COSMIC Func-\ntion Point (CFP) [46], have been usually used as a cost driver to\nestimate traditional software development effort [4, 14, 16, 17, 53].\nThe advent of Agile Software Development (ASD) methodologies\n[19] has shifted the focus towards estimating the effort of develop-\ning smaller unit of software, like a new feature or change. In these\ncases, FSM methods are not easy to use [38] and another measure,\nnamely Story Point (SP), has become popular in the context of ASD\n[56]. SP is a relative unit that represents an intuitive mixture of\ncomplexity and required effort of a user story (a.k.a. issue) [8, 53]. 1\nHowever, previous studies have shown that the accuracy of the\nSP estimate is sensitive to the practitioners’ expertise, and thus,\nprone to bias. According to Usman et al. [55], who surveyed 60\nengineers experienced in Agile Effort Estimation, the estimates\nof around half of the Agile teams were inaccurate by a factor of\n25% or more. Using inaccurate SP could result in iteration mis-\nmanagement and wrong prioritization of tasks, which in turn can\nlead to customer dissatisfaction or even project failure. Moreover,\nsince human-expert estimated SP has been used as a cost driver to\ntrain automated estimation models [24, 33, 35, 39, 54, 58] or as a\nprediction target [2, 10, 20, 29, 34, 40, 43, 49, 50], researchers and\npractitioners need to be aware if they are using inaccurate SP as\nthis might impact the accuracy of these models.\nPrevious case studies have provided discordant results on\nwhether SP can accurately capture software size and effort [10,\n26, 37, 38], and to date there is not enough empirical evidence on\nthis matter. Our work aims to fill this gap by carrying out a thor-\nough large-scale empirical study investigating the extent to which\n1A user story is a user-valued functionality which is specified in the form of one or\ntwo sentences in the everyday language of the user.\n183\n\n\nESEM ’22, September 19–23, 2022, Helsinki, Finland\nTawosi, Moussa, and Sarro\nusing Story Point reflects the effort needed to develop a user story\n(i.e., issue development time). To this end, we analyse 37,440 user\nstories coming from 37 Agile software projects tracked with Jira\n[1], which are available in the TAWOS dataset [48]. To the best of\nour knowledge this is the largest empirical study to date to investi-\ngate the relationship between SP and effort in Agile open-source\nprojects. In particular, we aim to answer the question What is the\nrelationship between the SP estimated for a given issue and its ac-\ntual development time?. Since developers do not always record the\nactual time they spent on the development of an issue [48] in the\nissue tracking system, we compare three different proxies for the\ndevelopment time computed using the issue changelog2 in order to\nanswer the question: To what extent can we approximate the actual\ndevelopment time as reported by the developers?. Furthermore, since\none would expect issue development time to be proportionate to\nthe story point assigned to a certain issue, we also aim to answer\nHow consistent is the assignment of SP throughout a project?.\nThe results of our empirical study show that among the three\nproxies, there is one which more closely reflects the development\ntime as recorded by the developers, namely the InProgress develop-\nment time. Moreover, we found that the correlation between this\nissue development time and human-expert estimated SP is medium\nor low for 93% of the projects we investigated. These results are in\nline with those we obtained using the recorded development time\nrather than the proxy and they highlight that SP is not an accurate\nindicator of the software development effort. Moreover we found\nthat the human-expert estimation is not consistent throughout the\nprojects. Although SP can remain useful for agile teams to orga-\nnize and plan their iterations, these results raise awareness that\nthe inaccuracy observed in the SP might be carried out into those\nautomated effort estimation models that use SP as a cost driver\nto predict issue development time. Moreover, recent studies have\nproposed the use of machine learning approaches to predict SP for\nissues based on historical human-estimated SP. This means that\nsuch approaches learn to imitate human-expert estimations at best\nwhich in itself might be misleading of the actual effort needed to\nrealise an issue. Further work is needed to understand the extent to\nwhich the use of inaccurate SP impact automated effort estimation\nmodels, and whether the use of development time (actual or proxies\nfor it) can provide the engineers and managers with more reliable\nand accurate models.\nThe rest of this paper is organised as follows. Section 2 provides\nsome background for those readers who are not familiar with soft-\nware size and effort measures, and issue tracking systems. Sections\n3 and 4 present the design and results of our empirical study, re-\nspectively. Section 5 discusses previous work most related to ours.\nFinal remarks and future work are discussed in Section 6.\n2\nBACKGROUND\nIn this section we briefly introduce the most common software\nfunctional size and effort measures proposed in the literature. We\nalso give some background on Jira, the issue tracking system used\nby the projects analysed in this study, and describe three proxy\nmeasures for issue development time [48].\n2The history of changes in the issue’s attributes.\n2.1\nSoftware Size Measures\nAlbrecht was the first to introduce a disciplined method for mea-\nsuring software product size, called Function Point Analysis (FPA),\nbased on the functionality the software product is built to deliver\nto the customer [5]. Soon after, he showed that there is a strong\ncorrelation between Function Points and the final effort of a soft-\nware [6]. Although FPA was designed to measure software from\nthe domain of business applications [47], it is still widely applied\nin the software production industry [15].\nCOSMIC3 Function Point (CFP) method belongs to the second\ngeneration of software functional size methods [46]. CFP also takes\nnon-functional requirements into consideration, and is suitable for\na broader range of application domains including, but not limited\nto, business applications, web applications, mobile applications,\nreal-time software, and service-oriented software [4, 14, 18, 47].\nIn the context of Agile software development, practitioners have\nintroduced and used Story Point (SP) as an Agile specific software\nsize measurement unit [12]. Unlike FPA and CFP, SP do not follow\na method of measurement, therefore, developers use them as a\nrelative measure to keep the relative difference of stories in size by\nassigning a point value to each user story. One common approach\nto determine the story point value of a user story is to select one\nof the smallest stories in the Backlog4 and assign it with one story\npoint. Then the more complex and larger user stories get more\npoints considering their size [12]. So any user story that is assigned\ntwo SP is twice as large as a user story that is assigned one SP. SP\nestimations need to be consistent throughout the project.\n2.2\nJira Workflow and Issue Development Time\nJira [1] is a widely-used issue tracking system that supports Agile\ndevelopment [31]. Using Jira, the development teams can record\ntheir estimated story point and the time taken for the development\nof the issue.\nAlthough Jira Software has provided teams with specific fields\nto record the actual effort (i.e., time) spent on an issue, usually the\ndevelopers do not use this feature to log their work. Thus, identify-\ning the actual time spent to develop an issue might be challenging.\nNonetheless, previous work [10, 48] derived an approximation of\nthe actual development time from the transitions recorded in the\nchange-log of the issues in the Jira repository.\nJira Workflow: In Jira projects, issues transition through stages\nof work —from creation to completion— following a path. This path\nis called workflow. Figure 1 shows a generic Jira workflow that\ncould be used to track issue transitions and calculate approximate\ntime spent on issue development.\nThe life cycle of an issue starts at the time of its creation (the\ngrey circle in the workflow). When the issue is considered to be\ndeveloped, its status changes into a To Do state (1). This is when\nthe issue gets assigned to a developer. When the developer starts\nworking on an issue, he/she changes its status to In-Progress (2).\nThe developer is given the option to stop working (3) and restart it\nagain (2) at any time.\n3Common Software Measurement International Consortium\n4Backlog is a breakdown of work containing an ordered list of user stories that an\nAgile team maintains for a project.\n184\n\n\nOn the Relationship Between Story Points and Development Effort in Agile Open-Source Software\nESEM ’22, September 19–23, 2022, Helsinki, Finland\nResolved\nTO DO\nStart Working\nStop Working\nResolved\nIN PROGRESS\nClose\nSet Resolution\nAutomatically\nReopen\nDONE\nReopen\nClosed\nRESOLVED\n2\n3\n4\nassigned\nCreated\n1\n5\n7\n8\n6\nFigure 1: A generic Jira workflow.\nWhen the development is finished, the developer changes the\nissue’s status to Done (4), and Jira automatically populates the\nResolution field (5). Also, the developer can set the Resolution\nfield any time during the life cycle of the issue. The Resolution\nfield can be populated with one of the several labels predefined\nin the workflow (usually but not necessarily with statuses defined\nwithin the Done category), e.g., Fixed, Completed, Closed, Delivered,\nInvalid, Duplicate, Won’t Fix, Rejected, and Cancelled, depending\non the project and issue type.\nJira recognises an issue as Resolved if the Resolution field is pop-\nulated. By default Resolved means that the issue is in a closed state\nand no more work is needed to be done. But in many workflows\nResolved is not an ultimate state. For instance, in a custom work-\nflow, once the issue is resolved there might be an inspection which\ndecides if the solution provided is sufficient and/or correct —the\nreview process— before the issue can be closed (6). If the solution\nis not accepted, the issue will then be reopened and it would need\nto be addressed again (7). Ultimately, an issue might be reopened\nafter it has been closed (8), although this is rare.\nIssue Development Time: The workflow is typically specific\nto the work processes within an organization/team. Indeed, Jira pro-\nvides organizations with the ability to create customised workflows\nand statuses for each project and issue type. This makes it difficult\nto create a general method to calculate the time by observing issue\ntransitions. However, a custom status defined in a custom workflow\nhas to belong to one of these three categories: To Do, In-Progress,\nand Done. Jira mandates the use of these categories and employs\nthem internally to identify the column under which each issue\nshould be listed in the software task board. Therefore, one can base\nthe time calculation upon these three categories. Specifically, using\nthe status categories, one can identify the transitions of the issues\nbetween the time they were set to be in progress, stopped progress,\nor accomplished.\nBased on the above workflow, we have defined the following\nthree proxies for issue development time.\nIn-Progress Time is defined as the duration in which an issue\nhas been in the “In-Progress” status. In most projects the “In-Progress”\nstatus is used by developers to mark the time that they spend on\nthe implementation of an issue. Hence, In-Progress Time might not\ninclude any time spent on testing, reviewing or discussion.5\nEffort Time is defined as the duration in which an issue has been\nin any of the statuses categorised as In-Progress. This definition can\nbe interpreted as a more realistic proxy for the effort since it includes\ntime spent for implementation, testing, reviewing, discussions, etc.,\nas the Effort Time considers all the time that an issue spends under\nany status from the In-Progress category.6\nResolution Time is defined as the duration required for an issue\nto be resolved. As we can see in Figure 1, an issue status can be\nset to Resolved at any point in its life cycle. This definition aims at\ncapturing the amount of time it takes for an issue to be resolved. To\nthis end, we consider the duration between the time an issue was\ncreated until it is Resolved [48]. This is the definition used in the\nliterature to measure the time to fix an issue [23, 28, 41]. This proxy\nslightly differs from the one used in the work of Choetkiertikul et\nal. [10], as it also takes into account the time between the creation\nof the issue and the first time it was set to an In-Progress status.\nWhereas, the proxy used by Choetkiertikul et al. considers the\nduration between the time an issue was first set to an In-Progress\nstatus and the time that it was resolved.\n3\nEMPIRICAL STUDY DESIGN\nIn this section, we describe the research questions posed in our\nstudy and the dataset, methods and statistical tests used to answer\nthese questions.\n3.1\nResearch Questions\nStory point, as a measure of effort, is expected to have a positive\ncorrelation with the actual time needed to realise a software.\nIn this study we aim at investigating the correlation between\nestimated story point and the actual effort. As, the actual effort is\nrarely recorded in an issue report, we analysed three proxies for\nthe development time based on the Jira workflow as described in\nSection 2.2. Therefore, our first research question assesses which\nof these proxies are a good approximation of the actual effort.\nRQ1. Approximating Issue Development Time: To what ex-\ntent can proxies be used to approximate the development time logged\nby the developers?\nOnce we assess whether these proxies provide a satisfactory\napproximation, we move to investigate the correlation between\nstory point and the actual effort (i.e., each of the three proxies\nproposed):\nRQ2. Correlation: What is the relationship between an issue’s\nstory point and its development time?\nTo answer this question we use three widely known correlation\nstatistics to verify the relationship between the SP and each of the\nthree proxies we used for approximating the development time (see\nSection 2.2).\n5Note that there is always a status named In-Progress within the In-Progress category,\nteams can add other statuses into this category based on their issue ecosystem. For\ninstance, in a project which has In-Progress, Test, and Review statuses defined in the\nIn-Progress category, the issue may transition from one status to another until it passes\nall the required stages before it is closed (i.e., set to Done). This is showed by a recursive\narrow for the In-Progress status in Figure 1.\n6To identify the category of each status in each project, we queried the metadata of\neach project’s repository by using the REST API provided by Jira [48].\n185\n\n\nESEM ’22, September 19–23, 2022, Helsinki, Finland\nTawosi, Moussa, and Sarro\nSP is a relative measure by definition, and the relativeness refers\nto the amount of the work that one story point represents. It can\ndiffer from project to project and from team to team. This rate (i.e.,\none story point ratio with respect to the amount of the effort in\nperson-hour) might be affected by aspects such as the experience\nof the team, the programming language and the technology used\nfor development. This rate is used to compute the productivity\nof the team and to make the story point scale specific to each\nteam. However, within a project, the estimation team should remain\nconsistent throughout the project with respect to the unit of work\nthat a story point represents. In other words, the amount of work\nconsidered for a story point should be kept the same until the end\nof the project and all the issues should be measured with that same\nunit. Nevertheless, keeping this rate consistent is challenging for\nany team. This phenomenon justifies the rationale for our third,\nand last, research question, which emphasises on the variance of\nthe time for each SP:\nRQ3. SP Consistency: How consistent is the assignment of story\npoint throughout a project?\nTo answer this question, we rely on a visual representation to\nidentify any deviation between the actual data and an ideal trajec-\ntory of consistency in SP estimation, derived from the data itself as\nfurther explained in Section 3.2.\n3.2\nMethodology\nTo answer RQ1, we compare the Timespent value with the time as\nmeasured by each of the proxies for each issue, and compute the\nabsolute error one would commit had a proxy been used rather\nthan the actual value. Specifically, to measure the resemblance of\nthe three proxy measures to Timespent, we compute the Sum of\nAbsolute Errors (SAE) between each of the proxies and the Time-\nspent for the issues contained in each project. SAE is computed as\nfollows: SAE = Ín\ni=1 |Pi −TSi |, where n is the number of issues in\nthe project with reported Timespent values, TSi is the Timespent\nvalue for issue i, and Pi is each of the values of the development\ntime proxies for issue i, obtained from the TAWOS dataset [48].\nThe proxy with the minimum SAE is the most representative of the\nTimespent. Then we apply statistical significance tests on the distri-\nbution of each of the proxies against Timespent to verify whether\nthe difference between them is statistically significant. Specifically,\nwe used the Wilcoxon Rank-Sum test (a.k.a. Mann–Whitney U test)\n[30] to check for statistical difference. The confidence limit is ini-\ntially set to α = 0.05 and is corrected for multiple hypotheses using\nthe standard Bonferroni correction (α/K, where K is the number\nof hypotheses). To answer RQ1, we tested the following null hy-\npothesis: H0: The distribution of the Timespent is not different from\nthat of the proxy Pi. For those cases where the null hypothesis is\nrejected, the following alternative hypothesis is accepted: H1: The\ndistribution of the Timespent is different from that of the proxy Pi. To\nmeasure the effect size of the difference, we use Vargha Delaney’s\nˆA12 measure, which is a standardised non-parametric effect size\nmeasurement, to assess how meaningful the difference between the\ntwo distributions is [7]. According to the Vargha Delaney’s effect\nsize, if the two distributions are very similar ˆA12 = 0.5. Respectively,\nan ˆA12 closer to 1 means that the two distributions are not similar.\nThe effect size is considered small for 0.6 ≤ˆA12 < 0.7, medium for\n0.7 ≤ˆA12 < 0.8, and large for ˆA12 ≥0.8, although these thresholds\nare not definitive [52].\nTo answer RQ2, we apply three correlation statistics to our data:\nthe Pearson r correlation coefficient [32], the Spearman’s ρ rank\ncorrelation [45], and the Kendall’s τ rank correlation [27]. The Pear-\nson correlation test measures the linear correlation between two\nvariables, while the Spearman’s and Kendall’s correlation tests are\nstatistics used to measure the ordinal association between two sam-\nples and assess how well the relationship between two variables can\nbe described using a monotonic function [9]. Unlike the Pearson’s\nr which considers the value of the data points, the Spearman’s ρ\nand Kendall’s τ work with ranks of data points which makes them\nless sensitive to strong outliers that lie in the tails of both samples\n[13]. All three correlation statistics range from +1 to −1, where +1\nindicates a perfect correlation and −1 indicates a perfect inverse\ncorrelation. A non-correlation is indicated by a 0. Although both\nSpearman’s ρ and Kendall’s τ measure rank correlation, they can-\nnot be compared directly with one another since they have different\nscales. Gilpin [22] describes the ratio of ρ to τ to be almost 1.5 for\nmost of the range. The two get close to each other as their magni-\ntude increases towards the limits (i.e., both approaching +1 or −1)\nand when they both approach zero. We use the Cohen’s standard\n[11, 25] for interpreting the correlation coefficients to determine the\nstrength of the relationship. Based on this, correlation coefficients\nbetween 0.10 and 0.29 represent a small association, coefficients\nbetween 0.30 and 0.49 represent a medium association, and coeffi-\ncients equal to or greater than 0.50 represent a large association.\nTo perform the correlation we used the cor.test method available\nin R version 4.0.1. Both the Spearman’s and Kendall’s correlation\nstatistic implementations used in this study can handle ties in the\ndata points.\nTo answer RQ3, we group all the issues that have been assigned\na same story point of value <X> (i.e., story point <class X>) in a\nsame class. Then we analyse the boxplots of the time spent on each\nissue for the distinct classes to understand if the distribution of\ntime in story point classes is a normal one, which would indicate\na normal distribution of the error for story point estimation in\neach class. To investigate consistency, we observe the median point\nin the distribution of development time per SP class. We use the\nmedian point as it is not affected by the extreme values. For a\nproject with inconsistent SP estimations, the median development\ntime will be affected (misplaced from ideal trajectory) due to many\nmiss-classifications of smaller or bigger tasks in a specific SP class;\nor from another point of view, issues estimated to have the same SP\ndo not agree on the same (or similar) development time. In an ideal\nscenario, the median of the distribution of the development time in\nstory point classes should have a linear relationship with the value\nof the story point. For instance, the median of the development\ntime for issues assigned with story point five should be five times\nlarger than the median of the development time for issues assigned\nwith story point one. Should this linear relationship hold for all\nof the issues in story point classes, we can assert that story point\nestimations are consistent throughout a project. To test this we\nshow the trajectory of this linear relation to visualise the degree of\nthe consistency.\n186\n\n\nOn the Relationship Between Story Points and Development Effort in Agile Open-Source Software\nESEM ’22, September 19–23, 2022, Helsinki, Finland\n3.3\nDataset\nWe sample data from the TAWOS dataset version 1.0 [48], which is\na collection of diverse open-source Agilesoftware projects. These\nprojects have been mined from several different repositories main-\ntained using Jira Issue Tracking System (ITS) [1]. The TAWOS\ndataset is publicly available in the form of a relational database\nand contains more than half a million issues from 44 projects. We\nused SQL queries to sample issues from this database7. Below we\ndescribe in detail, how we sample the set of projects investigated\nin this study.\nTo answer our first research question (i.e., RQ1), we analyse\nthose projects from the TAWOS dataset that have recorded the\nactual development time in the Timesent field of Jira. Hence, we\nselected all the issues that are resolved (i.e., we filter out all those\nthat are not addressed), and have the Timespent field populated.\nThen, we removed all issues having a Timespent value lower than\ntwo minutes as done in previous work [57], to reduce noise in the\ndata. After applying such a filtering, we retained all those projects\nwith at least 100 issues each. This resulted in a sample of 9,806\nissues from 15 projects. Descriptive statistics of this set, which is\nused to answer RQ1, are provided in Table 1a.\nAs most of the issues recorded either one of the Timespent or SP,\nin order to answer RQ2 and RQ3 we also sampled another set of\nissues.8 To this end we filtered out from the TAWOS data all those is-\nsues that are not addressed, and those that have been assigned with\nSP less than 1 and greater than 100, as done in previous work [10], to\nreduce the presence of data that is not relevant to the purpose of our\nempirical study. Moreover, we noticed that a considerable number\nof the issues in some of the projects have a proxy time equal to zero.\nAfter a careful manual inspection, we found that there are issues\nwhich never transitioned to an In-Progress status and thus they had\nbeen closed immediately after being created or opened. These cases\nmay correspond to issues where developers had already worked on\nthe issue before tracking the corresponding record in Jira created\nfor the mere purpose of recording issues. In order to reduce the bias\nintroduced by these cases, and in accordance with the filter used for\nRQ1, we removed all the issues with In-Progress time less than two\nminutes, which corresponded to a total of 34.47% of the issues sam-\npled from the TAWOS dataset. Furthermore, we filtered out issues\nwith outlying values of In-Progress time to minimise the effect of\nextreme values in our results.9 We therefore retained projects with\nat least 100 issues after filtering out unwanted ones, which left us\nwith 58.33% of the initially sampled data corresponding to a total\nof 28, 608 issues from 32 projects (equal to 44.11% of all the issues\nwith recorded SP in 32 projects under investigation). To identify the\noutliers, we used the Interquartile Range (IQR). The IQR, which is\nequal to the difference between the 75th and 25th percentiles of the\ndistribution of the data points, is multiplied by 1.5 and the resulting\nvalue is subtracted from and added to the first and third quartiles,\nrespectively, to get the lower and upper fences (a.k.a. Tukey fences).\n7The queries used to sample the data are publicly available in our online appendix\n[51].\n8Since RQ1 aimed at examining the approximation of the three proxies to the recorded\nTimespent values, and there, the analysis is independent from the SP values, therefore,\nwe can use a different sample for RQ2 and RQ3 without loss of generality.\n9Note that we did not filter out issues with regards to their development time proxy val-\nues from the sample used in RQ1, since the aim of RQ1 is to examine the approximation\nof the proxies to the recorded Timespent values.\nThe data points falling outside the lower and upper fences are con-\nsidered outliers and, hence, removed from the dataset. The resulting\ndata has been used to answer RQ2 and RQ3. Descriptive statistics\nof this sampled dataset can be found in Table 1b.Note that a total\nof 621 issues from the open-source data sampled for RQ1 are also\nin the sample used for RQ2 and RQ3. Therefore, the total number\nof unique user stories sampled from the TAWOS dataset is equal to\n37,440 extracted from 37 different open-source projects.\n3.4\nThreats to Validity\nTo mitigate construct validity threats we use data from real-world\nprojects, which have been carefully curated and used in previous\nwork [48–50]. The story point values are predicted by human-\nexperts and recorded in the Jira issue repository. However, the\nvalues we use as the actual time are extracted from the issue\nchange-log, based on the issue transitions recorded in the reposi-\ntory throughout the development process. We are aware that these\ntime-values might not accurately represent the actual effort spent\non developing an issue, and we mitigated this threat by considering\nthree different approximations (i.e., In-Progress time, Effort time,\nand Resolution time), each capturing different aspects of the actual\neffort. We also used the actual development time recorded for issues\nto investigate the extent to which these proxies resemble the actual\ndevelopment time. Data points which are likely to be noisy, such as\nissues that have not been fully resolved or issues with less than 2\nminutes of recorded development time, were filtered out from the\ndataset before any analysis, as described in Section 3.3.\nUsing proxies of the development time instead of the actual\ntime might also be a threat to the internal validity of this study.\nIn other words, the low correlation between SP and the proxies\nmight be because of an unrepresentative proxy and not the expert\nmisestimation. We mitigate this threat by conducting the same\ncorrelation analysis with the actual development time recorded by\nthe developers where this was available.\nWith regard to the conclusion validity, we used three well-known\ncorrelation statistics and reported the corresponding p-value. To\ninvestigate the similarity of the proxies to the actual development\ntime, we used the Sum of Absolute Errors and examined the statisti-\ncal difference between the absolute error distributions by applying\nthe Wilcoxon Rank-Sum test with all required assumptions checked,\nfollowing best practice for effort estimation studies [42].\nTo mitigate external validity threats we used a large set of 37\nprojects which differ in size, application domain, programming\nlanguage, and development team. Although we used such a diverse\ndataset, all the projects are open-source and the results might not\nbe generalizable to other contexts.\n4\nEMPIRICAL STUDY RESULTS\nThis section presents the results we have obtained in answering\nthe research questions described in Section 3.1.\n4.1\nRQ1. Approximating Issue Development\nTime\nTable 2 shows the SAE values computed for the three proxies with\nrespect to the Timespent value. We can observe that, among the\nthree proxies under study, In-Progress Time has the smallest error\n187\n\n\nESEM ’22, September 19–23, 2022, Helsinki, Finland\nTawosi, Moussa, and Sarro\nTable 1: List of projects we analysed for RQ1 (a) and RQs 2-3 (b). The total number of issues in each project is shown in the\nTotal Issues column. Before Filter shows the original number of issues extracted from the TAWOS dataset [48] and After Filter\nshows the number of issues remaining after the filtering process as explained in Section 3.3. The other columns show summary\nstatistics for SP, Timespent and its proxies.\n(a)\nRepository\nProject\nKey\nTotal Issues\nIssues with Timespent\nTimespent (minutes)\nIn-Progress Time (minutes)\nEffort Time (minutes)\nResolution Time (minutes)\nBefore Filter\nAfter Filter\nMin\nMax\nMean\nMedian\nSD\nMin\nMax\nMean\nMedian\nSD\nMin\nMax\nMean\nMedian\nSD\nMin\nMax\nMean\nMedian\nSD\nCrowd\nCWD\n4,311\n222\n220\n2\n4,800\n468.36\n240\n729.33\n0\n1,401,053\n13,258.70\n44.5\n101,509.06\n0\n1,401,053\n17,341.50\n2,122.5\n102,618.99\n7\n2,724,297\n185,866.38\n20,465.5\n436,488.49\nJira Software Cloud\nJSWCLOUD\n11,702\n255\n244\n30\n2,220\n383.70\n300\n368.79\n0\n219,603\n2,873.85\n118.5\n14,352.33\n0\n220,762\n2,878.60\n118.5\n14,424.37\n0\n1,968,722\n52,691.43\n5,750\n253,296.60\nJira Software Server\nJSWSERVER\n12,862\n262\n257\n30\n3,600\n394.01\n180\n535.28\n0\n192,687\n4,030.39\n0\n17,679.50\n0\n192,687\n4,030.39\n0\n17,679.50\n0\n1,434,770\n46,633.84\n5,998\n127,654.65\nJira Server\nJRASERVER\n44,165\n990\n981\n5\n24,622\n298.01\n120\n941.64\n0\n216,201\n2,167.55\n0\n12,139.01\n0\n248,570\n3,345.93\n0\n18,248.28\n0\n4,387,661\n145,363.86\n11,644\n416,982.46\nBamboo\nBAM\n14,252\n524\n521\n5\n8,460\n392.57\n240\n694.73\n0\n2,278,726\n11,284.45\n288\n103,415.46\n0\n455,765\n12,845.10\n4,253\n30,649.89\n1\n2,635,279\n78,653.98\n20,072\n222,346.01\nClover\nCLOV\n1,501\n106\n106\n2\n4,801\n605.87\n240\n906.33\n0\n259,569\n15,619.25\n191.5\n42,533.41\n0\n260,743\n21,726.25\n7,364.5\n43,420.66\n0\n1,350,621\n85,252.67\n30,289.5\n171,444.39\nAtlassian\nFishEye\nFE\n5,533\n634\n612\n2\n8,782\n265.36\n112\n638.94\n0\n4,771,423\n14,706.04\n94.5\n195,220.17\n0\n4,771,423\n38,411.01\n11,178.5\n257,383.61\n6\n4,797,823\n167,097.20\n34,210\n536,889.66\nAppcelerator\nTitanium Mobile Platform\nTIDOC\n3,059\n714\n711\n5\n11,040\n307.01\n120\n626.55\n0\n579,203\n7,839.01\n276\n32,121.50\n0\n579,203\n10,971.92\n1,399\n38,699.95\n0\n2,463,988\n131,868.77\n25,699\n303,406.05\nLsstcorp\nData management\nDM\n26,506\n191\n190\n5\n24,000\n934.08\n480\n1,847.11\n0\n1,695,993\n29,921.76\n1,445\n145,880.95\n0\n1,849,990\n54,467.54\n8,724.5\n222,795.99\n0\n1,850,092\n72,405.15\n19,166.5\n223,627.89\nSonatype\nNexus\nNEXUS\n9,912\n1,356\n1,348\n2\n4,560\n189.26\n90\n327.95\n0\n495,244\n2,310.59\n0\n21,501.77\n0\n495,244\n2,370.53\n0\n21,529.01\n0\n5,068,853\n42,930.46\n4,658\n202,515.39\nTalend Data Quality\nTDQ\n15,315\n2,054\n2,053\n10\n18,960\n764.44\n480\n1,144.12\n0\n165,637\n5,119.09\n1,174\n11,622.38\n0\n916,623\n42,361.35\n11,562\n94,348.85\n1\n4,002,454\n179,687.61\n42,085\n438,753.36\nTalend Data Preparation\nTDP\n5,670\n219\n193\n10\n6,660\n723.85\n300\n1,027.93\n0\n137,132\n7,632.41\n1,604\n15,791.27\n0\n162,678\n25,879.64\n16,936\n29,052.18\n1\n903,339\n106,585.19\n60,215\n141,153.18\nTalend Data Management\nTMDM\n9,137\n1,650\n1,648\n3\n5,760\n541.33\n360\n625.69\n0\n1,031,320\n5,019.47\n1,459.5\n27,701.47\n0\n1,231,065\n43,492.23\n19,125\n87,717.66\n0\n2,902,016\n98,187.55\n27,414\n228,291.61\nTalend Big Data\nTBD\n4,624\n193\n191\n5\n5,700\n712.36\n360\n943.00\n0\n152,267\n4,539.72\n1,131\n12,736.24\n0\n1,751,027\n103,679.79\n56,685\n192,330.37\n263\n1,751,054\n119,209.97\n53,074\n227,411.31\nTalendforge\nTalend Enterprise Service Bus\nTESB\n15,985\n436\n178\n60\n2,760\n268.66\n180\n289.30\n0\n2,391,610\n16,848.47\n1,472\n179,097.87\n0\n2,391,610\n31,680.99\n1,490\n241,500.85\n0\n2,480,801\n25,164.78\n5,380.5\n186,888.34\nTotal\n184,534\n9,806\n9,453\n(b)\nRepository\nProject\nKey\nTotal Issues\n# Issues with SP\nStory Point\nIn-Progress Time (minutes)\nBefore Filter\nAfter Filter\nMin\nMax\nMean\nMedian\nStD\nMin\nMax\nMean\nMedian\nStD\nJira Software Cloud\nJSWCLOUD\n11,702\n318\n185\n1\n20\n4.19\n3\n3.58\n2\n21,931\n5,212.30\n2,945\n5,267.85\nConfluence Server\nCONFSERVER\n42,324\n662\n362\n1\n13\n3.03\n3\n1.73\n2\n24,847\n4,511.35\n1,559.5\n5,922.60\nJira Software Server\nJSWSERVER\n12,862\n351\n208\n1\n20\n4.19\n3\n3.51\n2\n18,864\n4,696.80\n2,831\n4,749.70\nBamboo\nBAM\n14,252\n528\n302\n1\n20\n2.47\n2\n2.18\n2\n20,524\n4,104.67\n1,455.5\n5,048.14\nAtlassian\nClover\nCLOV\n1,501\n387\n146\n1\n20\n3.48\n2\n4.09\n2\n30,293\n5,700.54\n2,848\n6,655.53\nMesos\nMESOS\n10,157\n3,272\n1,157\n1\n13\n3.32\n3\n2.08\n2\n39,861\n6,311.93\n1,616\n8,922.99\nApache\nUsergrid\nUSERGRID\n1,339\n487\n162\n1\n8\n2.62\n3\n1.41\n2\n21,657\n4,905.17\n2,950.5\n4,898.76\nTitanium Mobile Platform\nTIDOC\n3,059\n1,297\n628\n1\n40\n4.28\n3\n4.14\n2\n46,257\n6,771.88\n1,553\n10,171.09\nAptana Studio\nAPSTUD\n8,135\n890\n302\n1\n40\n7.92\n8\n5.13\n3\n6,915\n1,222.96\n340\n1,618.00\nAppcelerator Studio\nTISTUD\n5,979\n3,406\n1,918\n1\n34\n5.69\n5\n4.14\n2\n5,243\n817.76\n182\n1,159.13\nThe Titanium SDK\nTIMOB\n22,059\n4,665\n1,753\n1\n21\n5.56\n5\n2.70\n2\n14,255\n1,927.80\n240.5\n3,137.66\nAppcelerator\nAppcelerator Daemon\nDAEMON\n313\n242\n131\n1\n99\n9.57\n8\n11.03\n2\n26,184\n3,403.00\n379\n6,141.23\nDNN Tracker\nDotNetNuke Platform\nDNN\n10,060\n2,594\n1,122\n1\n14\n2.18\n2\n1.46\n2\n9,553\n1,371.70\n199\n2,268.66\nBlockchain Explorer\nBE\n802\n373\n239\n1\n13\n3.01\n3\n1.77\n2\n33,120\n8,387.12\n5,754\n8,061.10\nFabric\nFAB\n13,682\n636\n235\n1\n24\n2.85\n2\n2.71\n2\n64,394\n11,464.59\n7,021\n13,734.58\nIndy Node\nINDY\n2,321\n681\n438\n1\n13\n3.21\n3\n1.73\n2\n38,453\n9,357.24\n7,190.5\n8,693.04\nSawtooth\nSTL\n1,663\n966\n646\n1\n8\n2.38\n2\n1.30\n2\n40,392\n11,375.65\n8,799\n9,660.83\nHyperledger\nIndy SDK\nIS\n1,531\n720\n418\n1\n13\n3.91\n3\n2.13\n2\n22,384\n4,986.39\n2,917\n5,339.56\nLsstcorp\nLsstcorp Data management\nDM\n26,506\n20,664\n9,019\n1\n100\n6.16\n3.2\n9.58\n2\n105,126\n18,290.56\n8,083\n24,287.98\nLyrasis\nLyrasis Dura Cloud\nDURACLOUD\n1,125\n666\n243\n1\n13\n2.05\n2\n1.55\n2\n24,555\n4,253.40\n1,363\n5,881.14\nCompass\nCOMPASS\n1,791\n499\n275\n1\n8\n3.43\n3\n1.73\n3\n50,363\n10,583.59\n4,351\n13,403.21\nC++ driver\nCXX\n2,032\n224\n105\n1\n4\n1.35\n1\n0.65\n2\n14,917\n2,458.21\n1,098\n3,587.64\nMongoDB Core Server\nSERVER\n48,663\n784\n418\n1\n42\n2.53\n2\n2.76\n2\n18,577\n3,081.26\n1,056\n4,419.22\nMongoDB\nEvergreen\nEVG\n10,299\n5,402\n1,674\n1\n8\n1.94\n2\n1.10\n2\n10,949\n2,116.95\n1,155\n2,742.38\nMule\nMULE\n11,816\n4,170\n2,105\n1\n21\n4.95\n4\n3.52\n2\n22,982\n5,195.96\n3,179\n5,394.67\nMulesoft\nMule APIkit\nAPIKIT\n886\n473\n284\n1\n13\n3.14\n3\n2.30\n2\n17,755\n3,522.79\n1,640\n4,242.36\nSonatype\nNexus\nNEXUS\n9,912\n1,845\n421\n1\n15\n1.56\n1\n1.22\n2\n11,467\n2,054.11\n518\n2,811.87\nSpring\nXD\nXD\n3,707\n3,705\n1,602\n1\n24\n3.37\n3\n2.48\n2\n20,942\n4,160.57\n1,680.5\n5,038.44\nTalend Data Quality\nTDQ\n15,315\n1,843\n1,151\n1\n40\n5.20\n5\n4.28\n2\n27,457\n6,161.18\n3,945\n6,663.57\nTalend Data Preparation\nTDP\n5,670\n813\n473\n1\n18\n2.24\n2\n1.72\n2\n48,922\n10,373.10\n7,181\n11,139.64\nTalend Data Management\nTMDM\n9,137\n297\n177\n1\n8\n2.42\n2\n1.60\n3\n20,046\n4,795.11\n2,996\n4,781.31\nTalendforge\nTalend Enterprise Service Bus\nTESB\n15,985\n1,000\n309\n1\n13\n2.28\n2\n1.51\n2\n47,937\n11,166.68\n8,370\n11,427.15\nTotal\n326,585\n64,860\n28,608\nfor all projects. However, while this indicates that In-Progress Time\nis the most representative of Timespent, the magnitude of the abso-\nlute errors shows that all three proxies have large differences with\nTimespent. This is due to the fact that the Timespent field, which\nstores the aggregated amount of time spent on the development\nof the issue, is computed as the sum of the work hours logged by\nthe developers on the issue. The proxies obtained from the TAWOS\ndataset are the aggregation of the duration between points in the\ntimeline for an issue’s status change, so they take into account the\nidle time that a developer might pause working but not change\nthe status. For example, if an issue’s status remains unchanged for\na week but a developer works five hours a day on the task, they\nmay log 25 hours for Timespent, while the proxies would take into\naccount the number of days (in progress, or to resolution) in order\nto measure development time. Hence, the proxy may be multiple\ntimes greater than the actual Timespent. However, this difference\nin magnitude does not affect the correlation results.\nOverall, considering all limitations of getting a close approxima-\ntion of the actual effort in open source projects, these proxies are\nthe most representative we could obtain from the data.\nIn order to verify whether the differences between Timespent\nand each of the three proxies are statistically significant, we revert\nto the Wilcoxon test. The results of this test are presented in Table\n2 (last three columns). Since we are interested in any difference be-\ntween each pair of distribution sets, we use a two-sided alternative\nhypothesis, therefore a comparison of Pi vs. Pj results in the same\np-value as the comparison of Pj vs. Pi. As revealed by the results,\nthe difference in the distributions of Timespent and In-Progress is\n188\n\n\nOn the Relationship Between Story Points and Development Effort in Agile Open-Source Software\nESEM ’22, September 19–23, 2022, Helsinki, Finland\nTable 2: RQ1. Difference between Timespent and the three proxy measures for development time (i.e., In-Progress Time, Ef-\nfort Time, and Resolution Time) in terms of Sum Absolute Error (SAE) and significance statistical tests (effect size shown in\nbrackets).\nProject\nSAE with Timespent\nTimespent vs.\nIn-Progress Time\nEffort Time\nResolution Time\nIn-Progress Time\nEffort Time\nResolution Time\nCWD\n2,909,699\n3,796,762\n40,790,437\n<0.001 (0.40)\n<0.001 (0.60)\n<0.001 (0.95)\nJSWCLOUD\n666,291\n667,450\n12,776,147\n0.285 (0.47)\n0.285 (0.47)\n<0.001 (0.80)\nJSWSERVER\n1,024,283\n1,024,283\n11,903,893\n<0.001 (0.23)\n<0.001 (0.23)\n<0.001 (0.72)\nJRASERVER\n2,230,661\n3,389,772\n142,334,494\n<0.001 (0.26)\n<0.001 (0.25)\n<0.001 (0.92)\nBAM\n5,784,351\n6,565,644\n40,777,437\n0.361 (0.52)\n<0.001 (0.68)\n<0.001 (0.96)\nCLOV\n1,641,549\n2,261,458\n8,973,869\n0.872 (0.49)\n<0.001 (0.67)\n<0.001 (0.93)\nFE\n8,961,125\n23,375,131\n102,102,251\n0.968 (0.50)\n<0.001 (0.86)\n<0.001 (0.99)\nTIDOC\n5,501,120\n7,703,434\n93,548,470\n0.085 (0.53)\n<0.001 (0.61)\n<0.001 (0.96)\nDM\n5,578,740\n10,194,085\n13,588,070\n0.043 (0.56)\n<0.001 (0.77)\n<0.001 (0.91)\nNEXUS\n3,114,794\n3,193,031\n57,646,533\n<0.001 (0.31)\n<0.001 (0.32)\n<0.001 (0.85)\nTDQ\n9,568,980\n85,743,427\n367,355,182\n<0.001 (0.54)\n<0.001 (0.82)\n<0.001 (0.96)\nTDP\n1,358,593\n4,863,758\n20,432,471\n<0.001 (0.67)\n<0.001 (0.91)\n<0.001 (0.98)\nTMDM\n7,521,187\n70,814,422\n160,926,609\n<0.001 (0.65)\n<0.001 (0.92)\n<0.001 (0.96)\nTBD\n784,328\n19,667,618\n22,633,044\n0.020 (0.57)\n<0.001 (0.99)\n<0.001 (0.99)\nTESB\n2,963,686\n5,602,799\n4,438,478\n<0.001 (0.70)\n<0.001 (0.72)\n<0.001 (0.80)\nsignificant in eight out of 15 projects, all with small or negligible\neffect size, except for the TESB project for which the effect size is\nmedium. However, the difference between Timespent and the Effort\ntime proxy is significant in 14 out of 15 cases, with JSWCLOUD\nbeing the only exception given that the recorded Effort time is very\nclose to In-Progress time in most of the issues belonging to this\nproject. Moreover, the difference is significant for all the cases when\ncomparing Timespent with the Resolution time. Out of 30 cases of\nstatistical tests on Effort Time and Resolution Time, 19 cases show\na large effect size, three cases a medium effect size, and 7 cases a\nsmall or negligible effect size.\nAs a result, we only consider In-Progress in our subsequent\nresearch questions given that it is the most representative of Time-\nspent compared to the other two proxies.\n4.2\nRQ2. Correlation\nThe results of three correlation statistics (RQ2) are shown in Table 3.\nWe also reported the p-value for each correlation coefficient.\nFor all projects, Kendall’s τ is consistent with the Spearman’s ρ\nin the scale and confidence level. However, if we consider Gilpin’s τ\nto ρ conversion table [22], we would expect a higher ρ. For example,\nin the case of the CONFSERVER project (see Table 3), Gilpin’s table\nmaps a τ = 0.26 to ρ = 0.38, while our data lead to a ρ = 0.34. The\nrationale behind this is the fact that Kendall’s τ is the proportion of\nthe concordant to discordant pairs while the Spearman’s ρ considers\nthe variance in the ranks. Hence, as we obtain a ρ smaller than\nexpected (indicated by ρ to τ rate) it shows the high variance in\nthe ranks of the data, to which Spearman is sensitive but Kendall is\nnot. This high variance in the ranks is a sign of misclassification\nof many issues by human-estimators in wrong SP classes, thus an\nerror in the estimation.\nThe Pearson correlation coefficient is lower than the Spearman’s\nρ for 24 projects (75% of the cases), which indicates that the relation\nbetween the story point and development time is not usually linear.\nAs we can observe, the correlation denoted by Spearman’s ρ for\nIn-Progress time is low in six out of 32 cases, medium in 21 cases and\nstrong for only five cases. The strongest positive correlation appears\nto be in projects DAEMON, INDY, JSWCLOUD, JSWSERVER, and\nTable 3: RQ2. Correlation results between SP and In-\nProgress Time (p-value in brackets). Medium and strong cor-\nrelations are highlighted in orange and red , respectively.\nProject\nIn-Progress Time Correlation with Story Point\nSpearman’s ρ\nKendall’s τ\nPearson r\nJSWCLOUD\n0.54 (<0.001)\n0.41 (<0.001)\n0.47 (<0.001)\nCONFSERVER\n0.34 (<0.001)\n0.26 (<0.001)\n0.26 (<0.001)\nJSWSERVER\n0.53 (<0.001)\n0.40 (<0.001)\n0.49 (<0.001)\nBAM\n0.35 (<0.001)\n0.28 (<0.001)\n0.35 (<0.001)\nCLOV\n0.45 (<0.001)\n0.34 (<0.001)\n0.44 (<0.001)\nMESOS\n0.40 (<0.001)\n0.30 (<0.001)\n0.35 (<0.001)\nUSERGRID\n0.20 (0.013)\n0.16 (0.008)\n0.12 (0.139)\nTIDOC\n0.48 (<0.001)\n0.36 (<0.001)\n0.27 (<0.001)\nAPSTUD\n0.38 (<0.001)\n0.30 (<0.001)\n0.40 (<0.001)\nTISTUD\n0.42 (<0.001)\n0.33 (<0.001)\n0.35 (<0.001)\nTIMOB\n0.28 (<0.001)\n0.22 (<0.001)\n0.23 (<0.001)\nDAEMON\n0.62 (<0.001)\n0.48 (<0.001)\n0.66 (<0.001)\nDNN\n0.32 (<0.001)\n0.25 (<0.001)\n0.27 (<0.001)\nBE\n0.19 (0.003)\n0.15 (0.002)\n0.21 (0.001)\nFAB\n0.49 (<0.001)\n0.38 (<0.001)\n0.36 (<0.001)\nINDY\n0.56 (<0.001)\n0.44 (<0.001)\n0.53 (<0.001)\nSTL\n0.41 (<0.001)\n0.32 (<0.001)\n0.39 (<0.001)\nIS\n0.49 (<0.001)\n0.38 (<0.001)\n0.43 (<0.001)\nDM\n0.49 (<0.001)\n0.36 (<0.001)\n0.42 (<0.001)\nDURACLOUD\n0.52 (<0.001)\n0.41 (<0.001)\n0.47 (<0.001)\nCOMPASS\n0.30 (<0.001)\n0.23 (<0.001)\n0.25 (<0.001)\nCXX\n0.27 (0.005)\n0.22 (0.006)\n0.36 (<0.001)\nSERVER\n0.49 (<0.001)\n0.37 (<0.001)\n0.29 (<0.001)\nEVG\n0.36 (<0.001)\n0.28 (<0.001)\n0.28 (<0.001)\nMULE\n0.48 (<0.001)\n0.36 (<0.001)\n0.49 (<0.001)\nAPIKIT\n0.37 (<0.001)\n0.28 (<0.001)\n0.30 (<0.001)\nNEXUS\n0.25 (<0.001)\n0.19 (<0.001)\n0.25 (<0.001)\nXD\n0.41 (<0.001)\n0.31 (<0.001)\n0.38 (<0.001)\nTDQ\n0.44 (<0.001)\n0.32 (<0.001)\n0.36 (<0.001)\nTDP\n0.36 (<0.001)\n0.27 (<0.001)\n0.23 (<0.001)\nTMDM\n0.18 (0.016)\n0.14 (0.015)\n0.23 (0.002)\nTESB\n0.33 (<0.001)\n0.26 (<0.001)\n0.24 (<0.001)\nMin\n0.18\n0.14\n0.12\nMax\n0.62\n0.48\n0.66\nMean\n0.40\n0.31\n0.35\nSD\n0.11\n0.08\n0.11\nDURACLOUD. Looking at the p-value of the Spearman’s ρ, we find\nthat the confidence level is above 99% for 30 out of the 32 projects\nunder study (94% of the cases).\n189\n\n\nESEM ’22, September 19–23, 2022, Helsinki, Finland\nTawosi, Moussa, and Sarro\nTable 4: RQ2. Correlation results between SP and Timespent\n(p-value in brackets). Medium and strong correlations are\nhighlighted in orange and red , respectively.\nProject\nTimespent Correlation with Story Point\nSpearman’s ρ\nKendall’s τ\nPearson r\nDM\n0.33 (<0.001)\n0.28 (<0.001)\n0.35 (<0.001)\nMDL\n0.34 (<0.001)\n0.27 (<0.001)\n0.11 (0.179)\nTDQ\n0.64 (<0.001)\n0.52 (<0.001)\n0.61 (<0.001)\nTMDM\n0.03 (0.729)\n0.03 (0.688)\n0.28 (0.004)\nAs a subsequent analysis we computed the three correlation\nstatistics on all the issues from the TAWOS dataset that have re-\nported both the SP and Timespent values. This results in a total\nof 697 issues from four projects (specifically, 128 issues from DM,\n303 issues from TDQ, 104 issues from TMDM, and 162 issues from\nMDL). Although the number of such issues is not prevalent, it gives\nus an indication of how much results of RQ2 can be resembled by\nactual development time values (i.e., Timespent).\nThe result of this correlation analysis is shown in Table 4. As we\ncan see, only one out of four projects showed a strong correlation\nwith respect to all three statistics. From the other three projects,\ntwo show a medium range Spearman’s ρ and one a medium range\nPearson’s r coefficient. For the rest of the cases (i.e., 50%) a low\ncorrelation is obtained between SP values and actual Timespent.\n4.3\nRQ3. Consistency\nFigure 2 shows the boxplots of the development time distribution\nfor each story point value for six sample projects.10. As previously\nexplained (Section 3.2), for an acceptable story point estimation\nerror, each of these boxplots should resemble a normal distribution,\nand the relation between the median of each box should be propor-\ntional to the value of the story point class. The ideal projection of\nmedian development time per each SP class is depicted by a line\nconnecting the diamonds in Figure 2. This projection is computed\nby multiplying the SP value with the median development time for\nall issues estimated to have one story point.\nFrom the boxplots, we can observe that this proportion does not\nhold for most of the classes. Besides, the distribution of each class\ntends to be heavily tailed. This observation is confirmed by the\nShapiro-Wilk test [36], which has revealed that the data is not nor-\nmally distributed for any of the projects [51]. Specifically, only for\nseven projects out of the 32 under study, the median development\ntime per SP class (depicted by median line inside each boxplot)\nfalls in the vicinity of the ideal projected value (for example see\nprojects CLOV and JSWCLOUD in Figure 2). For four other projects,\nthe projection line falls well above the actual median development\ntime (e.g., the BE project in Figure 2), indicating an over-estimation.\nWhile for the remaining 21 projects, the projection line falls well be-\nlow the actual median development time (e.g., the APIKIT, XD and\nMULE projects in Figure 2). This high number shows the tendency\nfor human-experts to generally underestimate the time required to\ncomplete a certain task.\nWe further analyse this phenomenon by fitting a regression line\nto the median development time of each class against the value of\n10Due to space, the boxplots for all projects can be found in our on-line appendix [51].\nTable 5: RQ3. Angles created between the X axis and the lin-\near regression fit for SP classes against Median In-Progress\ntime when only SP classes ≤5 are considered (Angle (SP≤5))\nand when all the classes are considered (Angle (SP≤100)),\nand the angle between the two (Difference).\nProject\nAngle (SP≤5)\nAngle (SP≤100)\nDifference\nJSWCLOUD\n40.41◦\n24.22◦\n16.19◦\nCONFSERVER\n48.35◦\n20.77◦\n27.58◦\nJSWSERVER\n41.12◦\n24.48◦\n16.64◦\nBAM\n57.40◦\n21.57◦\n35.83◦\nCLOV\n57.51◦\n22.65◦\n34.86◦\nMESOS\n54.04◦\n39.22◦\n14.82◦\nUSERGRID\n26.17◦\n38.78◦\n12.61◦\nTIDOC\n23.57◦\n23.73◦\n0.16◦\nAPSTUD\n-4.67◦\n5.71◦\n10.38◦\nTISTUD\n0.72◦\n7.78◦\n7.06◦\nTIMOB\n1.45◦\n6.79◦\n5.34◦\nDAEMON\n1.40◦\n11.14◦\n9.74◦\nDNN\n26.08◦\n7.92◦\n18.16◦\nBE\n48.00◦\n45.96◦\n2.04◦\nFAB\n78.33◦\n19.39◦\n58.94◦\nINDY\n66.94◦\n63.87◦\n3.07◦\nSTL\n65.47◦\n54.82◦\n10.65◦\nIS\n46.96◦\n25.49◦\n21.47◦\nDM\n75.48◦\n26.95◦\n48.53◦\nDURACLOUD\n56.49◦\n46.08◦\n10.41◦\nCOMPASS\n47.02◦\n59.73◦\n12.71◦\nCXX\n64.53◦\n64.53◦\n0.00◦\nSERVER\n30.39◦\n7.43◦\n22.96◦\nEVG\n36.15◦\n24.12◦\n12.03◦\nMULE\n29.20◦\n31.42◦\n2.22◦\nAPIKIT\n19.81◦\n20.73◦\n0.92◦\nNEXUS\n17.80◦\n23.40◦\n5.60◦\nXD\n31.91◦\n17.83◦\n14.08◦\nTDQ\n50.19◦\n11.11◦\n39.08◦\nTDP\n63.45◦\n-0.90◦\n64.35◦\nTMDM\n32.77◦\n37.86◦\n5.09◦\nTESB\n69.70◦\n7.97◦\n61.73◦\nthe story point. We then fit another regression line considering only\nthe classes of SP with values less than five. As the angle between\nthese two lines widens, the consistency between story point classes\nbecomes lower. In contrast, if these two lines are aligned together\nfor a project, we can say that the consistency of estimation in lower\nSP classes is maintained for higher SP classes. This is based on the\npremise that human experts are better at estimating smaller tasks\nthan the bigger ones. Plots of the regression fits for all projects can\nbe found in our online appendix [51].\nWe also report, in Table 5, the angles each of these lines cre-\nates with the x-axis as well as the deviation in the trajectory (i.e.,\ndifference between the two angles).\nWe can observe that the angle between the two lines is wider than\n12.5◦for more than half the projects (53% of the cases). This signifies\nthat there is a notable shift in the trajectory of the regression fit of\nthose projects taking into account the median development time\nfor the higher SP classes. Thus, the scale in which the issues in the\nhigher SP classes are estimated is not consistent with the issues\nestimated to be in the smaller SP classes. Therefore, based on these\nobservations, we recommend that development teams consider\nbreaking down bigger issues into smaller ones before they attempt\nto estimate the story point.\nIt is also worth noting that using data consisting of estimated\nSP to train a predictive model, would result in that model imitating\nhuman expert misestimates and therefore possibly achieving biased\nresults.\n190\n\n\nOn the Relationship Between Story Points and Development Effort in Agile Open-Source Software\nESEM ’22, September 19–23, 2022, Helsinki, Finland\n0\n100\n200\n300\n1\n2\n3\n4\n5\n8\n13\nStory Point\nIn Progress Time (hours)\nAPIKIT\n(a) APIKIT\n0\n250\n500\n750\n1000\n1\n2\n3\n4\n5\n7\n8\n11\n13\nStory Point\nIn Progress Time (hours)\nBE\n(b) BE\n0\n200\n400\n600\n1\n2\n3\n4\n5\n8\n13\n20\nStory Point\nIn Progress Time (hours)\nCLOV\n(c) CLOV\n0\n100\n200\n300\n1\n2\n3\n4\n5\n6\n7\n8\n10\n13\n14\n15\n16\n20\n24\nStory Point\nIn Progress Time (hours)\nXD\n(d) XD\n0\n100\n200\n300\n400\n1\n2\n3\n5\n8\n13\n20\nStory Point\nIn Progress Time (hours)\nJSWCLOUD\n(e) JSWCLOUD\n0\n100\n200\n300\n400\n1\n2\n3\n4\n5\n8\n10\n13\n16\n21\nStory Point\nIn Progress Time (hours)\nMULE\n(f) MULE\nFigure 2: Boxplots of the distribution of development time per SP class for (a) APIKIT, (b) BE, (c) CLOV, (d) XD, (e) JSWCLOUD,\n(f) MULE. The red line depicts a project-specific baseline, drawn based on the median development time for one SP.\n5\nRELATED WORK\nIn this section, we discuss previous studies relevant to ours that\n(1) analyse the relationship between human-expert estimated SP,\nfunctional size measure (such as FP and COSMIC) and development\ntime/effort; (2) use human-expert estimated SP as a cost driver for\nautomated Agile software effort estimation models; (3) use machine\nlearning models to predict the SP of user stories.\n5.1\nSoftware Size Measures and Agile\nDevelopment Effort\nThe first study assessing the relationship between SP and functional\nsize measures (FSM) was carried out in 2011 by Santana et al. [38].\nThe authors of this study quantitatively analysed the relationship\nbetween FP and SP in a case study involving 2,191 user stories from\n18 iterations of an Agile software projects developed by a private\ncompany. They found a strong positive correlation between SP and\nFP (Spearman’s ρ = 0.71). Subsequently, Huijgens and Solingen\n[26] replicated Santana et al.’s work on a different case study and\nfound a contrasting result. They gathered data from 14 iterations\nperformed by two teams (A and B) in a Dutch banking organization\nthat recorded estimations in SP, and computed the size in FP for all\niterations. The results of the Spearman’s rank correlation revealed\na medium (−0.36) and strong (−0.60) negative correlation between\nSP and FP for Teams A and B, respectively.\nSP has been also compared to COSMIC, and actual effort.\nSalmanoglu et al. [37] compared the correlation of SP and CFP\nwith the actual effort spent for Agile software development. They\ncarried out three case studies from three large Turkish companies\nproducing software solutions for security, financial, and telecommu-\nnication industries, reporting CFP, SP and the actual effort measured\nin person-hours. They plotted SP and CFP values against the actual\neffort to measure the linearity of functional size and actual effort,\nand observed a stronger correlation between CFP and actual effort,\nin comparison with SP.\nChoetkiertikul et al. [10] carried out a preliminary analysis on\nthe relationship between SP and development time on a set of 16\nopen-source projects mined form Jira repositories for a total of\n23,313 issues [10]. The main aim of their study, however, was to\nbuild a machine learning model to estimate SP. They computed\ndevelopment time from the issue changelog by considering the\nduration between the time the issue’s status was set to In-Progress\nand the time it was set to Resolved. This was regarded by the authors\nas the most representative proxy for the actual effort they were\nable to extract from the data with respect to the completion time\nof an issue. However, this definition also includes the waiting time\nbetween development stages as part of the development time. In\nthis paper, we adopt a similar proxy for the issue resolution time,\nand use two additional proxies for development time to take into\naccount the waiting time (which is usually considered part of the\ndevelopment time) as proposed by Tawosi et al. [48]. Choetkiertikul\net al. [10] found a positive correlation between the SP and their\nproxy for development time with a mean of 0.47 and 0.51 and a\nstandard deviation of 0.19 and 0.18, according to the Spearman’s\nrank and Pearson correlation, respectively. Our investigation on a\nlarger dataset showed a positive but weaker correlation than the one\n191\n\n\nESEM ’22, September 19–23, 2022, Helsinki, Finland\nTawosi, Moussa, and Sarro\npreviously found [10], specially based on the Pearson correlation\ncoefficient. Across the 32 projects investigated herein, we obtained\na mean value of 0.40 for the Spearman’s ρ and 0.35 for the Pearson’s\nr coefficient, with a standard deviation of 0.11 for both.\n5.2\nAutomated Story Point Prediction\nSeveral studies in the literature have used automated estimation\ntechniques to predict SP of issues in Agile software projects. Abra-\nhamsson et al. [3] used several features extracted from user stories,\nincluding their SP, to train a model which estimates SP for new\nstories. Porru et al. [34] built classification models to classify user\nstories into SP classes. Scott and Pfahl [40] used developer-related\nfeatures alongside the features extracted from user stories to es-\ntimate SP using machine learning. Soares [43] used Autoencoder\nNeural Networks to classify user stories based on the semantic dif-\nferences of their titles in order to estimate their SP size. Choetkier-\ntikul et al. [10] combined two deep learning architectures, to build\nan end-to-end prediction system for SP size of user stories, called\nDeep-SE. Abadeer and Sabetzadeh [2] used Deep-SE [10] for SP\nprediction of a closed-source project with 4,727 user stories. Tawosi\net al. [50] replicated Choetkiertikul et al.’s study [10] by evaluating\nDeep-SE on a larger dataset of open-source projects. In another\nstudy, Tawosi et al. [49], used a clustering-based method to estimate\nSP for issue reports aiming at improving the performance of Deep-\nSE. Marapelli et al. [29] built a model based on a tree-structured\nRNN with Convolutional Neural Network (CNN) to predict SP for\nuser stories. This model adopts a Bi-directional LSTM (BiLSTM)\nwhich improves Deep-SE’s prediction performance. More recently,\nFu and Tantithamthavorn [20] proposed GPT2SP, a Transformer-\nbased deep learning model for SP estimation of user stories and\nfound that this model outperforms Deep-SE.\n5.3\nStory Point as a Cost Driver for Agile Effort\nEstimation\nZia et al. [58] considered human-expert estimated story size and\nstory complexity to compute SP for user stories and they used it\nto estimate the actual effort and cost for software projects. They\nintroduced a regression-based model considering characteristics\nof agile development. The model was applied to 21 previously de-\nveloped small software projects and produced estimations with a\nmean absolute error of four days. Later, Popli and Chauhan [33]\nproposed a similar approach but evaluated the model on one small\nproject.\nUngan et al. [54], investigated the accuracy of multiple linear\nestimators and a simple Artificial Neural Network estimator on 10\nindustrial projects as a case study. All the projects had their actual\neffort and SP recorded by developers and their CFP were automati-\ncally approximated using a tool named CUBIT. Results showed that\nwhen the estimator uses CFP or SP, as independent variables, the\naccuracy of effort estimation is low or at most acceptable, and none\nof the two models is superior to the other.\nRaslan et al. [35] proposed a fuzzy logic technique-based effort\nestimation framework for user stories. The approach feeds expert\nestimated SP alongside other parameters as input to a trapezoidal\nmembership function to estimate the actual effort. The model is\nnot evaluated on any real data; however, the authors designed a\nframework based on the proposed model on MATLAB, to make it\nready for evaluation and possible adoption.\nSatapathy et al. [39] used a dataset of 21 projects from the work\nof Zia et al. [58] and evaluated the effort estimation accuracy of\ndifferent machine learning techniques, namely, Decision Trees (DT),\nStochastic Gradient Boosting (SGB), and Random Forest (RF). Based\non the results, the DT model underperformed the technique previ-\nously proposed by Zia et al. [58]. Whereas the SGB and RF models\nperformed better than it.\n6\nCONCLUSIONS\nWe have studied the relationship between human-expert estimated\nStory Point (SP) and the time required by the developers to realise a\ngiven issue (i.e., development time) on a large sample of open-source\nuser stories sampled from the TAWOS public dataset [48], which\nconsists of 37 software projects for different application domains,\ndiversified in size and characteristics, resulting in a total of 37,440\nunique issues.\nThe results of our empirical study showed that, among the three\nproxies for development time we studied herein, In-Progress time\nis the most representative of the actual development time recorded\nby the developers. When considering its correlation with human-\nexpert estimated SP we found that for the majority of the projects\nsuch a correlation is low (35%) or medium (58%). Analysing the\ncorrelation between SP and the actual development time unveiled\na similar outcome: SP showed a low (50%) or medium (25%) correla-\ntion with Timespent. We also found that majority of the investigated\nprojects (25 out of 32) lack consistent human-expert estimations for\nSP. The consistency starts to wear when the issues are estimated\nto be bigger than five points, thereby suggesting that human es-\ntimators are not accurate at assessing the size of the issues that\nneed five times or more effort than an issue worth a single story\npoint. To overcome this issue Agile teams can try to break-down all\ntasks/issues estimated to be bigger than five SP into smaller ones.\nThe above results provide empirical evidence that human-expert\nestimated SP might not be a good indicator for the issue develop-\nment effort of Agile open-source projects. This might render any\nmachine-learnt effort estimation model, which learns from human-\nexpert estimated SP, vulnerable to the same bias and its impact\nshould be taken into account in future work. It would be interest-\ning, for example, to assess if more accurate effort estimation models\ncan be obtained by using the development time instead of SP as\na cost driver. Moreover, future work could replicate our study by\nconsidering industrial projects to expand the understanding beyond\nthe open-source realm investigated herein. Also future work could\ninvolve expert certified FSM measurers to compute the FP and CFP\nof the user stories available in the TAWOS dataset, so that one could\ncarry out a large scale empirical study analysing the correlation\nbetween SP, FP, CFP, and actual development time.\nIn order to allow for replication and extension of our work, we\nmake our data and scripts publicly available [51].\nACKNOWLEDGMENTS\nVali Tawosi, Rebecca Moussa and Federica Sarro are supported by\nthe ERC grant no. 741278 (EPIC).\n192\n\n\nOn the Relationship Between Story Points and Development Effort in Agile Open-Source Software\nESEM ’22, September 19–23, 2022, Helsinki, Finland\nREFERENCES\n[1] [n.d.]. Jira | Issue & Project Tracking Software | Atlassian. https://www.atlassian.\ncom/software/jira\n[2] Macarious Abadeer and Mehrdad Sabetzadeh. 2021. Machine Learning-based Es-\ntimation of Story Points in Agile Development: Industrial Experience and Lessons\nLearned. In 2021 IEEE 29th International Requirements Engineering Conference\nWorkshops (REW). IEEE, 106–115.\n[3] Pekka Abrahamsson, Ilenia Fronza, Raimund Moser, Jelena Vlasenko, and Witold\nPedrycz. 2011. Predicting development effort from user stories. In 2011 Inter-\nnational Symposium on Empirical Software Engineering and Measurement. IEEE,\n400–403.\n[4] Silvia Abrahão, Lucia De Marco, Filomena Ferrucci, Jaime Gómez, Carmine\nGravino, and Federica Sarro. 2018. Definition and evaluation of a COSMIC mea-\nsurement procedure for sizing Web applications in a model-driven development\nenvironment. Information and Software Technology 104 (2018), 144–161.\n[5] Allan J Albrecht. 1979. Measuring application development productivity. In Proc.\nJoint Share, Guide, and IBM Application Development Symposium, 1979.\n[6] Allan J. Albrecht and John E Gaffney. 1983. Software function, source lines of\ncode, and development effort prediction: a software science validation. IEEE\ntransactions on software engineering 6 (1983), 639–648.\n[7] Andrea Arcuri and Lionel Briand. 2014. A hitchhiker’s guide to statistical tests\nfor assessing randomized algorithms in software engineering. Software Testing,\nVerification and Reliability 24, 3 (2014), 219–250.\n[8] Kent Beck and Martin Fowler. 2001. Planning extreme programming. Addison-\nWesley Professional.\n[9] Sarah Boslaugh. 2012. Statistics in a nutshell: A desktop quick reference. \" O’Reilly\nMedia, Inc.\".\n[10] Morakot Choetkiertikul, Hoa Khanh Dam, Truyen Tran, Trang Pham, Aditya\nGhose, and Tim Menzies. 2018. A deep learning model for estimating story points.\nIEEE Transactions on Software Engineering 45, 7 (2018), 637–656.\n[11] Jacob Cohen. 2013. Statistical power analysis for the behavioral sciences. Academic\npress.\n[12] Mike Cohn. 2005. Agile estimating and planning. Pearson Education.\n[13] Christophe Croux and Catherine Dehon. 2010. Influence functions of the Spear-\nman and Kendall correlation measures. Statistical methods & applications 19, 4\n(2010), 497–515.\n[14] S. Di Martino, Filomena Ferrucci, Carmine Gravino, and Federica Sarro. 2016.\nWeb Effort Estimation: Function Point Analysis vs. COSMIC. Information and\nSoftware Technology 72 (2016), 90–109.\n[15] Sergio Di Martino, Filomena Ferrucci, Carmine Gravino, and Federica Sarro. 2016.\nWeb effort estimation: function point analysis vs. COSMIC. Information and\nSoftware Technology 72 (2016), 90–109.\n[16] Sergio Di Martino, Filomena Ferrucci, Carmine Gravino, and Federica Sarro. 2020.\nAssessing the Effectiveness of Approximate Functional Sizing Approaches for\nEffort Estimation. Information and Software Technology 123 (2020).\n[17] F. Ferrucci, C. Gravino, P. Salza, and F. Sarro. 2015. Investigating Functional\nand Code Size Measures for Mobile Applications. In Proceedings of Euromicro\nConference on Software Engineering and Advanced Applications. 365–368.\n[18] Filomena Ferrucci, Carmine Gravino, Pasquale Salza, and Federica Sarro. 2015.\nInvestigating Functional and Code Size Measures for Mobile Applications: A\nReplicated Study. In Product-Focused Software Process Improvement, Pekka Abra-\nhamsson, Luis Corral, Markku Oivo, and Barbara Russo (Eds.). Springer Interna-\ntional Publishing, Cham, 271–287.\n[19] Martin Fowler, Jim Highsmith, et al. 2001. The agile manifesto. Software Devel-\nopment 9, 8 (2001), 28–35.\n[20] Michael Fu and Chakkrit Tantithamthavorn. 2022. GPT2SP: A Transformer-\nBased Agile Story Point Estimation Approach. IEEE Transactions on Software\nEngineering (2022).\n[21] Michael Gammage. 2011. Why Your IT Project May Be Riskier Than You Think.\nHARVARD BUSINESS REVIEW 89, 11 (2011), 22–22.\n[22] Andrew R Gilpin. 1993. Table for conversion of Kendall’s Tau to Spearman’s\nRho within the context of measures of magnitude of effect for meta-analysis.\nEducational and psychological measurement 53, 1 (1993), 87–92.\n[23] Mayy Habayeb, Syed Shariyar Murtaza, Andriy Miranskyy, and Ayse Basar Bener.\n2017. On the use of hidden markov model to predict the time to fix bugs. IEEE\nTransactions on Software Engineering 44, 12 (2017), 1224–1244.\n[24] Alaa El Deen Hamouda. 2014. Using agile story points as an estimation technique\nin cmmi organizations. In 2014 agile conference. IEEE, 16–23.\n[25] James F Hemphill. 2003. Interpreting the magnitudes of correlation coefficients.\n(2003).\n[26] Hennie Huijgens and Rini van Solingen. 2014. A replicated study on correlating\nagile team velocity measured in function and story points. In Proceedings of the\n5th International Workshop on Emerging Trends in Software Metrics. 30–36.\n[27] Maurice G Kendall. 1938. A new measure of rank correlation. Biometrika 30, 1/2\n(1938), 81–93.\n[28] Youngseok Lee, Suin Lee, Chan-Gun Lee, Ikjun Yeom, and Honguk Woo. 2020.\nContinual prediction of bug-fix time using deep learning-based activity stream\nembedding. IEEE Access 8 (2020), 10503–10515.\n[29] Bhaskar Marapelli, Anil Carie, and Sardar MN Islam. 2020. RNN-CNN MODEL:\nA Bi-directional Long Short-Term Memory Deep Learning Network For Story\nPoint Estimation. In 2020 5th International Conference on Innovative Technologies\nin Intelligent Systems and Industrial Applications (CITISIA). IEEE, 1–7.\n[30] Patrick E McKnight and Julius Najab. 2010. Mann-Whitney U Test. The Corsini\nencyclopedia of psychology (2010), 1–1.\n[31] Marco Ortu, Giuseppe Destefanis, Bram Adams, Alessandro Murgia, Michele\nMarchesi, and Roberto Tonelli. 2015. The jira repository dataset: Understanding\nsocial aspects of software development. In Proceedings of the 11th international\nconference on predictive models and data analytics in software engineering. 1–4.\n[32] K Pearson. 1895. Notes on Regression and Inheritance in the Case of Two Parents\nProceedings of the Royal Society of London, 58, 240-242.\n[33] Rashmi Popli and Naresh Chauhan. 2014. Cost and effort estimation in agile\nsoftware development. In 2014 international conference on reliability optimization\nand information technology (ICROIT). IEEE, 57–61.\n[34] Simone Porru, Alessandro Murgia, Serge Demeyer, Michele Marchesi, and\nRoberto Tonelli. 2016. Estimating story points from issue reports. In Proceedings\nof the The 12th International Conference on Predictive Models and Data Analytics\nin Software Engineering. 1–10.\n[35] Atef Tayh Raslan, Nagy Ramadan Darwish, and Hesham Ahmed Hefny. 2015.\nTowards a fuzzy based framework for effort estimation in agile software devel-\nopment. International Journal of Computer Science and Information Security 13, 1\n(2015), 37.\n[36] J Patrick Royston. 1982. An extension of Shapiro and Wilk’s W test for normality\nto large samples. Journal of the Royal Statistical Society: Series C (Applied Statistics)\n31, 2 (1982), 115–124.\n[37] Murat Salmanoglu, Tuna Hacaloglu, and Onur Demirors. 2017. Effort estimation\nfor agile software development: Comparative case studies using COSMIC func-\ntional size measurement and story points. In Proceedings of the 27th International\nWorkshop on Software Measurement and 12th International Conference on Software\nProcess and Product Measurement. 41–49.\n[38] Célio Santana, Fabiana Leoneo, Alexandre Vasconcelos, and Cristine Gusmão.\n2011. Using function points in agile projects. In International Conference on Agile\nSoftware Development. Springer, 176–191.\n[39] Shashank Mouli Satapathy and Santanu Kumar Rath. 2017. Empirical assessment\nof machine learning models for agile software development effort estimation\nusing story points. Innovations in Systems and Software Engineering 13, 2 (2017),\n191–200.\n[40] Ezequiel Scott and Dietmar Pfahl. 2018. Using developers’ features to estimate\nstory points. In Proceedings of the 2018 International Conference on Software and\nSystem Process. 106–110.\n[41] Reza Sepahvand, Reza Akbari, and Sattar Hashemi. 2020. Predicting the bug\nfixing time using word embedding and deep long short term memories. IET\nSoftware 14, 3 (2020), 203–212.\n[42] Martin Shepperd and Steve MacDonell. 2012. Evaluating prediction systems in\nsoftware project estimation. Information and Software Technology 54, 8 (2012),\n820–827.\n[43] Rodrigo GF Soares. 2018. Effort Estimation via Text Classification And Autoen-\ncoders. In 2018 International Joint Conference on Neural Networks (IJCNN). IEEE,\n01–08.\n[44] Ian Sommerville. 2016. Software Engineering GE. Pearson Australia Pty Limited.\n[45] Charles Spearman. 1961. The proof and measurement of association between\ntwo things. (1961).\n[46] Charles Symons. 2019. The COSMIC Method for Measuring the Work-Output\nComponent of Productivity. In Rethinking Productivity in Software Engineering.\nSpringer, 191–204.\n[47] Charles Symons, Alain Abran, Christof Ebert, and Frank Vogelezang. 2016. Mea-\nsurement of software size: advances made by the COSMIC community. In 2016\nJoint Conference of the International Workshop on Software Measurement and the\nInternational Conference on Software Process and Product Measurement (IWSM-\nMENSURA). IEEE, 75–86.\n[48] Vali Tawosi, Afnan Al-Subaihin, Rebecca Moussa, and Federica Sarro. 2022. A\nVersatile Dataset of Agile Open Source Software Projects. In Proceedings of the\n19th International Conference on Mining Software Repositories (MSR ’22). IEEE.\n[49] Vali Tawosi, Afnan Al-Subaihin, and Federica Sarro. 2022. Investigating the\nEffectiveness of Clustering for Story Point Estimation. In Proceedings of the 29th\nIEEE International Conference on Software Analysis, Evolution and Reengineering.\nIEEE, 816–827.\n[50] Vali Tawosi, Rebecca Moussa, and Federica Sarro. 2022. Deep Learning for Agile\nEffort Estimation Have We Solved the Problem Yet? arXiv:2201.05401 [cs.SE]\n[51] Vali Tawosi, Rebecca Moussa, and Federica Sarro. 2022.\nOnline Appendix\ncontaining Data and R Scripts for this study.\nhttps://github.com/SOLAR-\ngroup/SPvsDevelopmentEffort.git\n[52] Vali Tawosi, Federica Sarro, Alessio Petrozziello, and Mark Harman. 2021. Multi-\nobjective software effort estimation: a replication study. IEEE Transactions on\nSoftware Engineering (2021).\n193\n\n\nESEM ’22, September 19–23, 2022, Helsinki, Finland\nTawosi, Moussa, and Sarro\n[53] Adam Trendowicz and Ross Jeffery. 2014. Software project effort estimation.\nFoundations and Best Practice Guidelines for Success, Constructive Cost Model–\nCOCOMO pags (2014), 277–293.\n[54] Erdir Ungan, Numan Çizmeli, and Onur Demirörs. 2014. Comparison of functional\nsize based estimation and story points, based on effort estimation effectiveness in\nSCRUM projects. In 2014 40th EUROMICRO Conference on Software Engineering\nand Advanced Applications. IEEE, 77–80.\n[55] Muhammad Usman, Emilia Mendes, and Jürgen Börstler. 2015. Effort estimation in\nagile software development: a survey on the state of the practice. In Proceedings\nof the 19th international conference on Evaluation and Assessment in Software\nEngineering. 1–10.\n[56] Muhammad Usman, Emilia Mendes, Francila Weidt, and Ricardo Britto. 2014.\nEffort estimation in agile software development: a systematic literature review.\nIn Proceedings of the 10th international conference on predictive models in software\nengineering. 82–91.\n[57] Jie M Zhang, Feng Li, Dan Hao, Meng Wang, Hao Tang, Lu Zhang, and Mark\nHarman. 2019. A study of bug resolution characteristics in popular programming\nlanguages. IEEE Transactions on Software Engineering 47, 12 (2019), 2684–2697.\n[58] Shahid Kamal Tipu Ziauddin and Shahrukh Zia. 2012. An effort estimation model\nfor agile software development. Advances in computer science and its applications\n(ACSA) 2, 1 (2012), 314–324.\n194\n",
  "metadata": {
    "filename": "3544902.3546238.pdf",
    "num_pages": 12,
    "title": "On the Relationship Between Story Points and Development Effort in Agile Open-Source Software",
    "author": "Vali Tawosi, Rebecca Moussa, Federica Sarro",
    "subject": "-  Software and its engineering; "
  },
  "num_pages": 12,
  "timestamp": "2025-11-13T02:00:03.974836"
}
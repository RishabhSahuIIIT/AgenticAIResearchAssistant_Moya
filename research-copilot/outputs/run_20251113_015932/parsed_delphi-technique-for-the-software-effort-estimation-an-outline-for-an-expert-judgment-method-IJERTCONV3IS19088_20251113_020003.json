{
  "filename": "delphi-technique-for-the-software-effort-estimation-an-outline-for-an-expert-judgment-method-IJERTCONV3IS19088.pdf",
  "text": "Delphi Technique for the Software Effort \nEstimation -- An Outline for an Expert Judgment \nMethod \n \n \nDr. A M Nageswara Yogi, \nProfessor and Head, Department of MCA, \nT.John Institute of Technology, Bengaluru – 560083, India \n  \n \n \nAbstract – Models for estimating the software effort are \nessential to arrive at the total cost of software projects.  \nMajority of the available models are empirical and crude but a \npractical software developer can derive much benefit from \nthese methods.  To develop empirical equations for software \neffort estimation we need data from the large number of \ncompleted projects, which is difficult to obtain.  In addition \ndata from various projects cannot be compared since every \nsoftware \nproject \nwill \nbe \ndeveloped \nunder \ndifferent \nenvironments and technologies.  RAND Corporation developed \nthe Delphi method originally to forecast the impact of \ntechnology on warfare.  Delphi method uses expertise of \nexperienced persons to arrive at useful values for estimates.  A \ngroup of domain experts anonymously reply to a carefully \ncompiled list of questions.  Feedback in the form of statistical \nrepresentation of their responses will be sent to them in order \nto improve upon their earlier values.  The process repeats till \nconverged values for estimates are obtained.    If we replace the \nestimate by effort in person-months required for development \nof a software project to be undertaken then the Delphi method \ncan be applied for software estimation.   In this paper, we \npropose Delphi and Wideband Delphi methods for estimation \nof software projects by outlining various steps involved.  In \naddition we have discussed a few estimation methodologies.  \nWe observe that every software effort estimation method uses \nexpert’s knowledge in quantifying some of its parameters and \nhence judgment plays a very important role in software effort \nestimation. \n Keywords – Effort; Estimation; Expertise; Judgment; Delphi \nI. INTRODUCTION \n \nEstimation – “It is the mark of an instructed mind to rest \nsatisfied with the degree of precision which the nature of a \nsubject requires, and not to seek exactness where an \napproximation may suffice.” – Aristotle, 330BC   \nSystematic Software Development Process (SSDP) is \nthe key for successful development of software projects.  A \nsoftware project is said to be successful if it meets all the \nuser requirements including the functional and non-\nfunctional requirements and is delivered within the \nestimated cost and time.  To estimate the cost of a software \nproject it is required to accurately estimate effort in Person-\nMonths (PM) which is a dominant component of the budget \nfor a project to be undertaken.  But effort required to deliver \na software project, is dictated by many intangible \nparameters like project complexity, volatility and reliability \nof requirements and platforms, skills and capability of team \nmembers, etc.  In addition it is important to have the \nknowledge about the software that must be developed, \ndevelopment process, means, personnel and the user \nestablishment. \n \nTherefore, \nengineering \njudgment, \nexperience, scientific principles, and techniques are to be \nsynthesized to obtain the effort required to develop a \nsoftware project successfully.  We observe that the \nformulation of an effort estimation model by taking into \naccount many local factors like available skilled-manpower, \nwork-culture, environment, etc is one of the most important \nsteps in software project management.   \nEstimating cost of the effort required for a project to be \nundertaken constitutes the basis for Budgeting, Tradeoff and \nRisk Analysis, Project Planning and Control, Software \nImprovement and Investment Analysis, Business Planning \nand Scheduling.  Therefore, estimation plays a very \nimportant role in successful development of a software \nproject.  As per Wiki, the definition of Estimation is “the \nprocess of finding an estimate, or approximation, which is a \nvalue that is usable for some purpose even if input data may \nbe incomplete, uncertain, or unstable”.  Another definition \nof an estimate is prediction or a rough idea to determine \nhow much effort would take to complete a defined task [28].  \nAn estimate is also a forecast or prediction and approximate \nof what it would cost.   It is a rough idea about, how long a \ntask would take to complete?  We can also say that an \nestimate is an approximate computation of the probable cost \nof a piece of work.  We estimate to avoid overshooting \nbudget and time.  Before we start estimating the project \nactivities we must finalize all the requirements, if not \nanalyze the frequency of changes in the requirements.  In \naddition we have to make sure that infrastructure and \nmanpower with well laid down responsibilities are in place \nand well documented assumptions and forecasted risks with \npossible remedies [22]. \n    The estimation techniques are based on: \n1. The historical data and past experience: Historical \ndata can be formulized using Cost Estimation \nRelationships (CER) for system variables.  Limitation is \nthat these relationships do not include the technological \nchanges that may be adopted in future software \nInternational Journal of Engineering Research & Technology (IJERT)\nISSN: 2278-0181\nPublished by, www.ijert.org\nICESMART-2015 Conference Proceedings\nVolume 3, Issue 19\nSpecial Issue - 2015\n1\n\n\ndevelopments.  Past experience is very useful for \nvalidating the estimates.  Again here also changes in the \ntechnologies and the environments have to be taken into \naccount.  To have a positive effect on estimates, we \nmust adopt past experience along with training team \nmembers on the new technologies for the software \ndevelopment.  \n2. The available data from the documents:  Estimates \nfrom the available data from the documents will give \nsome insight to the estimates to be calculated based on \nthe present requirements.  Quantitative values from the \nprevious documents amalgamated with qualitative \nanalysis of the future requirements and the technologies \nwill improve the estimates. \n3. The assumption:  It is assumed that the requirements \nare well understood by both the user and the developer \nwithout any ambiguity and doubts. \n4. The calculated risks to be taken:  Here we would like \nto quote former US Deputy Assistant Secretary of the \nAir Force Lloyd Mosemann [29] who said: “Software is \nso vital to military system that, without it, most could \nnot operate at all. Its importance to overall system \nperformance, and the generally accepted notion that \nsoftware is always inadequate, makes software the \nhighest risk item and must be steadfastly managed... \nFailure to address risk has been the downfall of many \nDepartment of Defence acquisition programs.  The \nsystem component with the greatest inherent risk has \nhistorically been software.” \nII. SOFTWARE PROJECT ESTIMATION \nEstimation lays foundation for all project planning \nactivities and the planning provides road map for SSDP \nwhich is essential for successful completion of a software \nproject.  There is no simple way to make an accurate \nestimate of the effort required to develop a software project.  \nActual effort will be two to three times the effort estimated \nduring the initial stage of the project based on inadequate \ninformation in user requirements definition [30].  The \nStandish group [33] Chaos survey of over 350 organization \nand 8000 projects conducted during 1994 produced the \nfollowing results:  \n 16% of the projects undertaken were only successful, \n 31% cancelled before completion, \n 53% overrun budget and schedule, \nSubsequent Standish survey conducted in 2012 \nindicates that 39% of software projects undertaken were \nsuccessfully developed, but still leaves much opportunity \nfor further improvement.  A field survey by the Eindhoven \nUniversity of Technology [11] gives an overview of the \nstate of the art of the estimation and control of the software \ndevelopment projects in 598 Dutch organizations.  The \nconclusions are: \n 80% of the projects have overrun budget and schedule, \n 57% of the organizations do not use cost-accounting, \n 50% record no data on an ongoing projects, \n 35% of the organizations do not make an estimate, \n 50% is the mean overruns of the budget and the \nduration. \nOne of the most important reasons for such a situation \nis basically unrealistic software estimation.   Estimation \ndepends on many intangible parameters such as the software \nmay run on unfamiliar computers, use of new technology, \nthe people in the project may be inexperienced, \nunderestimation of quality of work and complexity, \nunrealistic specifications, etc.  This means that it is \nimpossible \nto \nestimate \nsoftware \ndevelopment \ncosts \naccurately during the early stage of a project since most of \nthe requirements are not clear.  At this stage an estimate is \nvery much required to bid the contract and we can obtain \nonly rough initial estimates.  The project manager can \nderive much benefit out of these initial estimates by using \nthem for budgeting, tradeoff and risk analysis, project \nplanning and control, software improvement investment \nanalysis etc.  Therefore, in literature we find lot of research \ntaking place in formulating cost estimation models suiting \nthe environments under which the software projects are \nbeing developed. \nCost estimation models can be classified into \nalgorithmic and non-algorithmic models.  In algorthmic \nmodels the costs are analyzed using mathematical formulae \nlinking costs and inputs with metrics [31] like Kilo \nDelivered Source Instructions (KDSI), Function Points, and \nObject Points etc to produce an estimate.  The formulae \nused in such formal models are developed from the analysis \nof historical data.  A basic algorithmic model can be \nrepresented by an equation of the type: \n    EFFORT (PM) = a (KDSI)b + c, \nwhere a, b and c are constants derived using the historical \ndata from the past  software projects.  Table 1 gives a few \nmodels with values of a, b and c. \nTABLE 1. ALGORITHMIC MODELS \nMethod \na \nb \nc \nBailey – Basil \n0.73 \n1.16 \n5.5 \nBoehm Simple method \n3.20 \n1.05 \n0.0 \nDoty \n5.288 \n1.047 \n0.0 \nHalstead \n0.7 \n1.50 \n0.0 \nSEL method \n1.4 \n0.93 \n0.0 \nWalston – Felix \n5.2 \n0.91 \n0.0 \nThe most popular algorithmic cost estimation model for \nsoftware projects is the COCOMO II developed by Barry \nBoehm and Ellis Harrowitz [8].  COCOMO is the \nparametric estimation model, which takes into account \nhistorical information as the base, making assumptions \nregarding changes, and extrapolating the information to the \npresent project.  The COCOMO model is an empirical \nmodel that was derived by collecting data from a large \nnumber of software projects.  These data were analysed to \ndiscover formulae using statistical approach for the best fit \nto the observations. These formulae link the size of the \nsoftware and product, project, team factors to the effort \nrequired.  COCOMO model has a long history from initial \ninstallation in 1981 [4] through a refinement tailored to Ada \nsoftware development [5] to COCOMO II [8]. \nInternational Journal of Engineering Research & Technology (IJERT)\nISSN: 2278-0181\nPublished by, www.ijert.org\nICESMART-2015 Conference Proceedings\nVolume 3, Issue 19\nSpecial Issue - 2015\n2\n\n\nTable 2 shows the basic COCOMO formula for three \ndifferent types of projects, whose complexity is simple, \nmoderate and embedded and M is the multiplier which \ndepends on product, computer, project and personnel \nattributes.  The accuracy of the estimations can be improved \nby calibrating the model to the local development \nenvironment by adjusting the weight-ages assigned to the \nvarious parameters of the model.   \nTABLE 2.  COCOMO FORMULAE \nDescription \nFormula \nProject \ncomplexity \nWell-understood \napplications developed by \nsmall teams. \nEFFORT (PM) = \n 2.4(KDSI)1.05M \nSimple \nMore \ncomplex \nprojects \nwhere team members may \nhave limited experience of \nrelated systems. \nFFORT (PM) =  \n3.0(KDSI)1.12M \nModerate \nComplex projects where the \nsoftware is part of a strongly \ncoupled complex of hw, sw, \nregulations and operational \nprocedures. \nEFFORT(PM) =  \n3.6(KDSI)1.20  M \nEmbedded \nThere are two broad approaches for non-algorithmic \ncost estimation – the top down approach and the bottom-up \napproach [25].  Top-down approach starts at system level.  \nWe start examining the overall functionality of the product \nand how that functionality is provided by interacting sub-\nfunctions.  The cost/effort of system-level activities such as \nintegration, configuration management and documentation \nare taken into account.   Bottom-up approach starts at \ncomponent level.  The system is decomposed into \ncomponents, and we estimate the effort required to develop \neach of these components.  Add all these component efforts \nto compute the effort required for the whole system \ndevelopment.  That is the bottom-up estimation models \ninvolve aggregating individual estimates for each task in the \nWork Breakdown Structure (WBS).  The top-down \napproach allows managers to consider all subsystems \nwhereas the bottom up approach allows for a more detailed \nassessment. \nIII. \nBASIC COST ESTIMATION MODEL \n    The cost of manufacturing any product or undertaking a \nproject is the sum of various costs factors such as: \n RDDTE (Research, Design, Development, Training and \nEvaluation) costs,  \n Acquisition cost which includes initial purchases, raw \nmaterial, equipments, spares, training etc,  \n Manufacturing costs include fuel/power, machinery and \nmanpower costs,  \n Initial maintenance costs like warranty which varies \nfrom one to five years from the date of installation,  \n Infrastructure and Marketing costs which include \nbuildings, \ndepots, \nmanpower \nand \ntraining \nof \nmaintenance personnel etc.   \nA basic cost estimation model identifies the number of \nunits to be produced and the total cost to produce them.  The \ntotal cost divided by number of units produced gives the \ncost per unit.  The manufacturer may add appropriate profit \nto the total production cost.  In an addition the cost of \nlogistics for transporting units to the desired places will be \nadded.  Usually RDDTE costs and the initial maintenance \ncosts up to warranty period are distributed over the number \nof items produced [19].  More the number of units \nproduced, the cost per unit will be reduced.   \nSoftware project cost estimation is no different than that \nof an engineering product except that the developed \nsoftware is visible only through its output, which has to be \nverified, validated and rigorously tested before the delivery \nof the software product. \nIV. \nSOFTWARE COST COMPONENTS \nSoftware cost components are the costs of Hardware, \nSoftware, Design, Development, Testing, Manpower, \nTravel, Training and Maintenance.  It is assumed that \nrequired hardware is available in the market.  Effort cost is \nthe dominant factor in most software projects which \nincludes the salaries of engineers involved in the project, \nsocial and insurance costs etc.  Effort cost must also include \nthe overheads such as costs of building, heating, lighting, \ncost of support staff such as accountants, administrators, \nsystem mangers and technicians, costs of networking and \ncommunications, costs of shared facilities like library, staff \nrestaurant, cost of social security and employee benefits \nsuch as pensions and health insurance.   \nTravel and training costs can be reduced by using \nelectronic communications such as e-mail, web and \nvideoconferencing.  Costs of buildings, heating and lighting \ncircuits are one time investment normally known as Capital \ncosts.  Recurring cost is the sum of salaries, power, \nmaintenance; housekeeping etc and is usually known as \nRevenue costs.  Usually revenue costs far exceed the capital \ncosts. \nV. \nWORK BREAKDOWN STRUCTURE \nWork Breakdonw Structure (WBS) based techniques \nare good for planning and control.  WBS was a standard of \nengineering practice in the development of both hardware \nand software.  In this technique the complex project is \ndivided into smallest pieces known as sub-functionalities \n(smallest components) which are considered as tasks.  A set \nof related sub-functionalities leading to a common feature \nforms a functionality.  A set of related functionalities makes \na sub-module.  A group of related sub-modules makes a \nmodule.   \nA software project consists of number of modules.  \nBreakdown of the work into smallest components must be \nreviewed to confirm that all functionalities are included in \nWBS.  Using such a breakdown of a project into micro level \nsub-functionalities we can easily figure out what are all the \ntasks need to be completed.  Estimating each of the tasks \nInternational Journal of Engineering Research & Technology (IJERT)\nISSN: 2278-0181\nPublished by, www.ijert.org\nICESMART-2015 Conference Proceedings\nVolume 3, Issue 19\nSpecial Issue - 2015\n3\n\n\nwould be easier than estimating overall complex project.  \nHence WBS is a way of organizing project components into \na hierarchy that simplifies the tasks for effort (cost) \nestimation and control the project scheduling.   \nMoreover, if probabilities are assigned to the costs \nassociated with each individual component of the hierarchy, \nan overall expected value can be determined from the \nbottom-up approach for total project development cost [2].  \nExpertise comes into play with this method in the \ndetermination of the most useful specification of the \ncomponents within the structure and of those probabilities \nassociated with each component.  By using the components \nfrom WBS, the project manager and team will have an idea \nwhether or not captured all the necessary tasks based on the \nproject requirements.  The WBS helps the project manager \nand the team to create the task scheduling and detailed cost \nestimation of the project.    \n    A software WBS actually consists of two hierarchies, one \nrepresenting the software product itself, and the other \nrepresenting the activities needed to build that product [4].  \nThe activity hierarchy Figure 1 [6] indicates the activities \nthose may be associated with the given software component.  \nThe product hierarchy Figure 2 describes the fundamental \nstructure of the software, showing how the various software \ncomponents fit into the overall system. \n     \n \n     \n \nFigure 1.   Activity Hierarchy WBS \nIn addition to effort estimation the other major use of \nthe WBS is cost accounting and reporting.  Each element of \nthe WBS can be assigned its own budget and cost control \nnumber, allowing staff to report the amount of time they \nhave spent working on any given project task or component. \nThis information can then be summarized for budget control \npurposes.   \nFinally if an organization consistently uses a standard \nWBS for all its projects, over the time it accrues a very \nvaluable database reflecting its software cost distributions.    \nThis data can be used to develop a software cost estimation \nmodels tailored to the organization’s own experience and \npractices.  While estimating the cost of each smallest \ncomponent, it is required to determine the level of \ncomplexity of each task such as very simple, simple, \naverage, moderate or complex.  When more than one person \nwith required skill level is available, then Project Manager \ncan assign a task by reasonably judging the requirements of \nthe task and the skill level of the person.   Later on project \nmanager should not repent that the task would have been \nassigned to the other person with similar skills. The \nparameter “Assign task to a team member” requires right \njudgment. Similarly, we can identify a number of \nparameters which needs judgment in software project \nmanagement. \nThe advantages of WBS are \n \nIn the WBS we generate all micro level tasks \n(smallest component) known as sub-functionalities to \nmacro level tasks (modules) of a software project to \nbe designed and developed.   Such a division of the \nwhole project into detailed tasks helps the Project \nmanager, Team and Customer to raise the critical \nissues during the initial development phase of the \nproject itself.  This helps to focus on the scope of the \nproject.  Discussions at this stage help to understand \nthe assumptions and clear ambiguities \n \n \n                                                                                    \n \nFigure 2.  Software Product WBS \n \nAn effective schedule and good budget plans can be \nmade since all micro level tasks related to software \nproject are available.  It helps in generating a \nmeaningful schedule and reliable budget. \n \nThe micro level details of tasks help to assign \nparticular module task to an individual.  This makes \nthe responsibility of team member very clear and so \nthe individual is accountable to complete the task. \nTeam members cannot escape from their duties as the \ndetailed task in WBS is available.  \n \nSince every member of the project team can \nunderstand the micro level tasks, it creates interest in \nthem.   This leads to increased participation and \ncommitment of all the team members. \nVI. \nPRICE-TO-WIN MODEL \nIn many cases the costs of many software projects must \nbe estimated using incomplete user requirements for the \nsystem.  Usually available information will be very little.  \nInitial cost estimation is required to propose budgetary \nrequirements.  Under these circumstances pricing to win is \ncommonly used strategy.  The notion of pricing to win may \nbe unethical and un-business like [25].  Here the project cost \nis agreed on the basis of outline proposal and based on the \ncustomer’s budget rather than resources and capabilities.  \nThe specifications are constrained by the agreed cost.  The \ncustomer and the developer must agree on acceptable \nsystem functionality.  The fixed factor is cost and not \nfunctional requirements.  The user requirements may change \nbut not the cost.    \nThe advantage is that the developer gets the project \ncontract and there is no need of experts’ opinions on any of \nDetailed Design \nSystem Engineering\nMaintenance \nProgram \nDevelopment Activities \nCode and Unit test \nSoftware Application \nComponent A \nComponent N \nComponent B \n Sub-component B1 \nSub-component B2 \nInternational Journal of Engineering Research & Technology (IJERT)\nISSN: 2278-0181\nPublished by, www.ijert.org\nICESMART-2015 Conference Proceedings\nVolume 3, Issue 19\nSpecial Issue - 2015\n4\n\n\nthe parameters.   The disadvantages are that this approach \ndoes not consider the possibility of projects incurring loss \nowing to scope creep or any other reason and the probability \nthat the customer getting the required system is small.  Costs \ndo not accurately reflect the work required [32].  There is no \nrequirement of any judgment playing any role in this model. \nVII. \n ANALOGY COSTING METHODOLOGY \nAn analogy costing model is a non-algorithmic costing \nmodel that estimates the cost of a project by relating it to \nanother similar completed project.  In this case the \nbenchmark will be the completed project similar in scope, \nsize, structure, environment, constraints, and functions to \nthe current project [32].  Estimations of the completed \nprojects will be used for reasoning and analogy to relate the \ncosts to be incurred for the reasonably similar components \nin the current project. Such estimation may be conducted \neither top-down or bottom-up.  This method has the \nadvantage of assessing costs based on actual project \nexperience rather than theoretical assumptions. However, \neffective estimation of the current project depends on the \nextent to which the completed project bears resemblance to \nthe current project.  The relationship between the similar \ncomponents will always be subjective and based on one’s \nown judgment.  Expert opinions can be obtained while \ncomparing the components.  However in software \ndevelopment, changes in technologies and environment are \nvery dynamic.   Analogy model will not take these changes \ninto consideration.  A corrective factor for changes in \ntechnologies and environment has to be derived using \njudgment again to make the estimate more accurate.  \nIf the WBS is available for the previouly completed \nproject to which we are comparing with project to be taken \nup then we can generate WBS for this project for \ncomparison.  This will also helps to validate the effort \nvalues obtained by analogy costing methodology. \nVIII. EXPERT JUDGMENT METHOD \nAn expert looks at long range future - fifteen or more \nyears distant (that is three five-year plans and after) in terms \nof probabilities of occurrences.  To an expert the future does \nnot appear as unique, unforeseeable or inevitable, but \nvisualizes a multitude of possible futures each associated \nwith certain objectively qualified subjective probabilities.  \nWith an advance vision of the possible futures and \nidentification of factors influencing their occurrences and an \nestimated extent of their influence it should become possible \nto manipulate or even design the future of one’s choice.  To \nbe able to predict the future, an expert must have relevant \nbasic information.  The quality of prediction would not only \ndepend upon the quality and quantity of information but also \non the expertise with which a futurist can handle the raw \ninformation.  The information may be three kinds – \nknowledge, speculation and opinion.   \nKnowledge is the kind of information which is highly \nsubstantiated by solid evidence of one kind or another.  \nSpeculation is the kind of the information that has little or \nno foundation.  Speculation lies on one end of the \ninformation spectrum which has knowledge on the other.  \nOpinion refers to the grey region that lies between \nknowledge on one end and speculation on the other. \nOpinion is the type of the information for which there is \nsome evidence, but not enough to say it is solid.  In the \nliterature on the long-range forecasting this kind of material \nis often called “wisdom” or “insight” or “informed \njudgment” or “experience”.  From the point of view of the \namount of supporting evidence these terms can be \nconsidered euph emisms for opinion.  The splitting \n/information spectrum into three distinct bands provides a \ncrude quantifiable scale for its processing and evaluation.  \nThe probabilities may be considered to be high for \nknowledge and low for speculation and for opinion it may \nbe considered to lie in between these two extremes.  The \nprobabilities that could be assigned to opinions would not \nonly depend upon the professional stature of the person \nexpressing them, but also upon his qualifications in the \nsubject under consideration.  In the area of opinions, \ntherefore, there is always a significant probability that what \nis being expressed may be incorrect.   \nA futuristic would very much wish to base his/her \npredictions on knowledge and would positively wish \nspeculation away.  But the very nature of long range \nforecasting where a number of parameters are foggy at best \nand where a large number of visualized situations in the not-\nso-clearly-known-domain \nfor \nexample \nrequirements \nspecification for software project to be developed are \nclouded leaves very little option for avoiding speculation \nentirely. \nMost of us are fond of algorithmic models, because \nmajority of us try to formulate mathematical models and \nhence we feel that the majority of research work carried out \nin the software cost estimation has been devoted to \nalgorithmic models.  However by an overwhelming \nmajority, expert judgment is the most commonly used \nestimation method.  Studies by Fred Heemstral [11] and \nVigder and Kark [27] reveal that 62% of estimators/ \norganizations use this intuition technique for software \nestimation.  \nExpertise-Based methods are useful in the absence of \nquantified and empirical data.  The expert judgment costing \nmodel makes the assessment of costs by leveraging the \nexperience of one or more Subject Matter Experts (SMEs) \n[32].  The method captures the knowledge and experience of \npractitioners seasoned within a domain of interest.  Experts \nprovide estimates based upon synthesis of the known \noutcomes of all the past projects to which they are privy or \nparticipated.  In addition experts will take into account the \ncurrent technologies and future trends while providing the \nestimates.  However Vidger and Kark indicated that in \ngeneral estimators did not refer to previous documents as it \nwas too difficult to access or the expert could not see how \nthe information would help in the accuracy of estimate.  The \nstudy reveals that majority of the experts tended to use their \nmemories of previous projects. Some may feel that expert \njudgment is simply a matter of guessing but Hughes [16] \nresearch indicates that experts tend to use a combination of \nInternational Journal of Engineering Research & Technology (IJERT)\nISSN: 2278-0181\nPublished by, www.ijert.org\nICESMART-2015 Conference Proceedings\nVolume 3, Issue 19\nSpecial Issue - 2015\n5\n\n\ninformal analogy approach when similar projects from the \npast are identified.  Therefore, the obvious drawback to this \nmethod is that an estimate is only as good as the expert’s \nopinion, and there is no way to test that opinion until it is \ntoo late to correct the damage if that opinion proves wrong.  \nYears of experience do not necessarily translate in to high \nlevels of competency.  Moreover, even the most highly \ncompetent individuals will sometimes simply guess wrong.  \nThese are some of the risks in this method. These risks can \nbe reduced by listing a group of experts to utilize their \nexpertise and by taking weighted average of their opinions.  \nAs we know that expert judgment is asking for an estimate \nof the task effort from some knowledgeable person about \neither the application or the development environment.  It is \noften required to refine estimates as and when additional \ndata becomes available.  Furthermore, we have to use \nrefined information in the next phases of an evolving project \nwhich is also a challenge for decision makers.  Therefore \njudgment plays a very important role in software estimation \n[24].  Judgment includes assumptions and logical thinking.  \nAssumptions are made based on the experience and \nexpertise.   \nA common expert judgment model is the Delphi \ntechnique.  The objective of this technique is the exploration \nof most creative and reliable ideas for decision making.  The \nmethod involves constituting an experts panel, conducting a \nsurvey where each expert states their opinion independently, \nfollowed by a controlled feedback to the experts, and \nrepeating the exercise multiple rounds until all the experts \ndevelop a consensus to identify a common cost estimate. \nIX. \nTHE DELPHI METHOD \nThe word Delphi refers to a place in Greece which was \nsupposed to confer predictive powers to the person visiting \nthe temple built there.  The Rand Corporation adopted this \nname for a procedure to obtain most reliable consensus of \nopinion of group of experts. The Delphi technique [14] was \ndeveloped at The Rand Corporation in 1948 originally to \nforecast the impact of technology on warfare (that is a way \nof making predictions about future events) - thus its name, \nrecalling the divinations of the Greek revelation of \narcheological find located on the southern edge of Mt. \nParnassus at Delphi.   \nThe Delphi Method is an information gathering \ntechnique in which expert opinions are systematically \nsolicited and collected.  This method is applicable when \nestimates have to be made based on informed judgment.  Its \nsalient features are \n \nAnonymity, \n \nStructured information flow,  \n \nIterations with regular and controlled feedback and \n \nStatistical group response.   \nIn its simplest form Delphi method eliminates the \ncommittee activity altogether thus reducing the influence of \ncertain psychological factors such as hollow opinions, the \nunwillingness to abandon publicly expressed opinions and \nbandwagon effect of majority opinions [13].   \nThe method entails a group of experts who \nanonymously reply to questionnaires and subsequently \nreceive feedback in the form of a statistical representation of \nthe \"group response,\" after which the process repeats itself.  \nThe goal is to reduce the differences in responses and arrive \nat something closer to expert consensus.  The expert surveys \nare conducted over multiple rounds until a consensus is \nreached.  After each round, controlled feedback is provided \nto the experts, which encourages convergence of thought.   \nA key point to note about the Delphi Technique is that \nwhile gathering thoughts, the participants do not know who \nthe other participants are? Hence, participants are not \ninfluenced by others taking part in the process.  By using \nthis method, we aim at obtaining both qualitative and \nquantitative \nresults. \n \nDelphi \nrepresents \na \nuseful \ncommunication device among a group of experts thus \nfacilitates the formation of group judgment [15].  The \nDelphi method is a top-down approach in aiming at the \nconvergence of judgmental values estimated by a number of \nexperts to obtain performance, cost of engineering products.   \nThis method is very useful during acquisition of \nsystems/ products, when more than one model of the same \nproduct/system are available in the open market.  The only \navailable information is the advertisement or brochures from \nthe manufacturers.  The Delphi method comes handy for us \nin deciding to select a model from available list.  Usually \nmany of us apply such a technique knowingly or \nunknowingly without formally recording the procedure.   \nThe Delphi method allows us to systematically record the \nexperts’ opinions for decision making. \nExpert judgment relies on experience, background and \ncommon sense of key people.  An expert might arrive at an \nestimate by considering the following: \n Similarity of the proposed project with completed ones. \n Earned experience from the previous projects. \n Available equipment including computers. \n Training given to the personnel who are going to be \ninvolved in the future projects. \nThe expert may be confident that the project is similar \nto the previous project but might have overlooked some \nfactors that make the new project significantly different.  It \nis also possible that expert making the estimate may not \nhave an experience with a project similar to the present one.  \nIn order to compensate these factors, group of experts are \nrequested to give the estimates.  This minimizes individual \noversights, personnel biases etc.  Disadvantage of group \nestimation is the effect of interpersonal group dynamics, \nnon-availability of authoritative figures in the group or the \ndominance of an overly assertive group member.  The \nDelphi technique can be used to overcome these \ndisadvantages.   This method replaces direct debate with \ncarefully designed program of sequential individual \ninterrogations \nbest \nconducted \nby \na \nquestionnaire \ninterspersed with information and opinion feedback derived \nby computed consensus from the earlier part of the program.  \nSome of the questions directed to the respondents (experts) \nmay for instance inquire into reasons for previously \nexpressed opinions, and collection of such reasons may then \nInternational Journal of Engineering Research & Technology (IJERT)\nISSN: 2278-0181\nPublished by, www.ijert.org\nICESMART-2015 Conference Proceedings\nVolume 3, Issue 19\nSpecial Issue - 2015\n6\n\n\nbe presented to respondent in the group, together with an \ninvitation to reconsider and possibly revive their earlier \nestimates.  Both the inquiry into reasons and subsequent \nfeedback of reasons adduced by others may serve to \nstimulate the experts to take into account considerations \nthey might through inadvertence have neglected and to give \ndue weight to factors they were inclined to dismiss as \nunimportant at first thought. \nThe mechanics of this technique maybe briefly \nexplained as follows.  A control group formulates a precise \nand unambiguous statement of the problem and prepares a \nquestionnaire and invites opinions from group of carefully \nchosen experts for the first round of estimates.  On receiving \nthe replies from these experts, which generally have a very \nwide spread, the control group determines the inter quartile \nrange that is the interval containing 50% of the responses.  \nThe processed first median response is fed back to experts \nwho are asked to reconsider their previous answers and \nrevise them if they so desired for second round.  If the fresh \nresponse of an expert is still outside the inter quartile range, \nhe is requested to state his reasons for thinking that answer \nshould be that much lower or that much higher than the \nmajority of the group. \nPlacing the onus of justifying relative extreme \nresponses on the experts has the effect of causing those \nwithout strong conviction to move their estimates closer to \nthe median, while those who feel they have good arguments \nfor a “deviationist” opinion, to retain their original estimate \nand defend it. \nIn the third round the estimates now a narrower spread \nare again surmised and the experts given a concise summary \nof reasons advanced in support of their extreme positions.  \nThey are once again requested to revise their second-round \nestimates, taking the proffered reasons into consideration \nand giving them whatever weight-age they thought was \njustified.  A respondent whose answer still remains outside \nthe inter quartile range would be requested to state why he is \nun-persuaded by the opposing arguments.   \nIn the fourth and generally the final round these \ncriticisms of the reasons previously offered are resubmitted \nto the respondents who are given the final chance to revise \ntheir estimates.  The median of these final responses may be \ntaken as responses representing the nearest thing to group \nconsensus. \nThere are many applications of Delphi method for \nengineering projects.  The very first application of the \nDelphi method carried out at RAND Corporation as \nillustrated in publication by Gordon and Helmer [12] was to \nassess the direction of long range trends, with special \nemphasis on science and technology and probable effects on \nsociety. The study covered six topics [12] Automation; \nPopulation \ncontrol; \nScientific \nbreakthroughs; \nSpace \nprogram; War prevention and Weapon system.   \nIn literature, I was not able to find either a case study or \nthe details regarding utilizing the Delphi method completely \nfor software effort estimation for a proposed software \nproject.  I have utilized Delphi method for finding the \nperformance of certain parameters of fighter aircraft to \ndevelop a model for comparative performance of certain \nfighter aircraft.   In this paper, I am proposing and outlining \nthe following steps to employ Delhi method for software \neffort estimation as follows:  \n1. Project Director will form a Control Team.  Control \nteam in consultation with the Project Director and Users \nprepares a System Definition document containing \nspecifications.  \n2. Control team analyses the System Definition document \nwhich includes both user requirements and system \nrequirements.  After the analyses, a meeting with the \nuser will be arranged to sort out any ambiguities, \nmisunderstandings, lack of clarity, confusions, and \namalgamated requirements etc to finalize the Software \nRequirements Specifications (SRS) document. \n3. Control team prepares a list of experts or estimators \nwho will have no contacts and appoints a coordinator \nwho is called as the facilitator.  While selecting the \nexperts the control team will consider their experience \nin software development, knowledge in application \ndomain.  It is better to select an odd number of experts \nwith a minimum of three for estimation of statistical \nparameters.   \n4. Usually the SRS document will be quiet big and it may \nnot be possible for experts to completely go through the \ndocument.  Therefore the control team carefully \nprepares a questionnaire for recording responses of the \nexperts to estimate effort, time, resources required for \nthe software project to be undertaken.  Along with the \nquestionnaire the control team will send a brief about \nthe project, listing the objectives, scope, timeline before \nwhich the experts are required to send their answers.  \nExperts may call for clarifications from the control \nteam for which the control team must be prepared to \nprovide the explanations. \n5. Experts study the specifications and complete the \nquestionnaire anonymously.  They may send queries to \nthe facilitator for clarifications.  After that they will \nsend estimates/ responses which we designate as the \nfirst round estimates. \n6. The control team based on the responses determines the \ninter quartile range, prepares and distributes the \nsummary of the responses including any unusual \nrationales noted by some experts and again sends this \ninformation to all the experts. \n7. Estimators complete another round of estimates, again \nanonymously, using the analyses of the results from the \nprevious estimates and feedback from the facilitator and \nthese results we call as second round estimates. \n8. On receiving the responses from the experts the control \nteam again determines the inter-quartile range.  Experts \nwhose estimates differ sharply from the group may be \nrequested to provide justification for their estimates.  \nControl team has to analyze the justification.  If \nInternational Journal of Engineering Research & Technology (IJERT)\nISSN: 2278-0181\nPublished by, www.ijert.org\nICESMART-2015 Conference Proceedings\nVolume 3, Issue 19\nSpecial Issue - 2015\n7\n\n\nacceptable the same can be sent to other estimators for \ntheir reactions. \n9. If only one or two experts differ sharply and reasons are \nnot justifiable in general or too adamant to change, then \nthey can be left out of the group.  \n10. We repeat steps 8 and 9 until a consensus is reached by \nthe panel of experts. \nThe process will be iterated for a minimum of four \nrounds; hopefully by this round the convergence might have \noccurred and the median of the final responses can be taken \nas answer representing the consciousness of the group of \nexperts.    \n11. The control team prepares a report of analysis of all the \nrounds. \n12. The project Director will arrange a presentation by the \ncontrol team to the Management to conclude the \nprocedure and for a possible decision making.   \n13. A letter expressing gratitude and thanks to all the \nexperts participated in the process must be sent by the \nHead of the organization. \nThe above process can be adopted if the experts \n/estimators are outside the organization.   \n If there are a number of experts within the organization \nitself, then the control team can have restricted group \ndiscussions with experts to focus on issues where estimates \nvary widely at the end of each cycle and try to reduce the \ndifferences.  However, control team has to see that there \nwill be no dominance of one expert over the other.  Even \nrationale for the widely varying estimates can also be \ndiscussed and the reasons cited can be analyzed giving equal \nweight for each of the experts.  It is to be noted that the \noriginal Delphi technique avoided group discussions. \nTherefore we have the Wideband Delphi method which \nallows limited group discussions. \nA. Wideband Delphi Method \nThe Wideband Delphi technique [4] accommodated \ngroup discussions in between the rounds.  It attempts to \ngather the opinions of a group of experts with the aim of \nproducing an accurate unbiased estimate.  It is a structured \ntechnique of expert judgment involving the following \nprocedure: \n1. Control team issues specifications and an estimation \nquestionnaire to the panel of experts. \n2. A group meeting of project stakeholders including the \nexperts will be conducted to discuss the software \nspecifications and estimation issues. \n3. Experts produce an independent first round estimates \nand confidentially submit to the facilitator of the control \nteam. \n4. The control team returns the estimates to the panel of \nexperts indicating the inter quartile range of estimates \nalong with the expert’s personnel estimate. \n5. Another group meeting of project stakeholders \nincluding the experts is conducted to discuss results and \nthe issues. \n6. Experts prepare revised independent estimates for the \nsecond round. \n7. We repeat the steps 3-6 until a consensus is reached by \nthe panel of experts. \nThe advantages of this method are that it removes the \npolitics from an estimate as the experts do not communicate \nabout their particular estimate and filters out extreme \nopinions making the estimate unbiased.  The group \ndiscussion of the stakeholders is particularly advantageous \nas it ensures that any estimation issues are not overlooked.  \nBoehm [4] indicates the effectiveness of the Wideband \nDelphi technique and stresses the importance of the group \nmeeting.  The disadvantages of this technique are that the \nmethod consumes lot of time and the panel of experts needs \nto have very good experience, patience to listen and not air \ntheir opinions during the discussions.  Also availability of \nexperts can not be guaranteed all the time \nThe Delphi technique has been successfully applied to a \nnumber of forecasting problems.  In majority of cases using \nDelphi technique approach, a convergence of opinions has \nin effect been observed.  In a few cases no convergence \ntowards relatively narrow interval of values took place; the \nopinions have polarized around two distinct values, so that \ntwo schools of thought regarding a particular issue seemed \nto emerge.  This may have been an indication that opinions \nwere based either on different sets of data or on different \ninterpretations of the same problem and the data.   \nThe quality of prediction in the Delphi technique \ndepends directly on the expertise of the participant \nrespondents.  If the problem is of an inter-disciplinary \ncharacter, it is possible to refine the method to introduce \nassignment of weight factors to expressed opinions.  An \nexpert may be requested to make a self-appraisal of his \nrelative competence in the matter under consideration.  The \nControl group on the basis of this self-appraisal process of \nexpert opinions determines the weighted median values. \nB. Convergence of estimates \n    Depending on the requirements and the situation, an \naverage of the estimates can be derived using arithmetical \naverage or statistical mode from the opinions of the experts.  \nThe highest estimate can be sent to the expert who has given \nthe lowest estimate for his opinions and comments and vice \nversa.   While sending the estimates, the control team will \nrequest to revise their earlier estimates.  This process may \nbring convergence of extreme answers received from the \nexperts.  \nC. Delphi Method for Software Risk and Schedule \nManagement \nWe can use the Delphi Method for any activity that \nrequires the convergence of expert thought, such as in Risk \nManagement and Scheduling.  For identifying risks drivers \nInternational Journal of Engineering Research & Technology (IJERT)\nISSN: 2278-0181\nPublished by, www.ijert.org\nICESMART-2015 Conference Proceedings\nVolume 3, Issue 19\nSpecial Issue - 2015\n8\n\n\nand classifying and quantifying them, we can develop a \nsuitable questionnaire so that the answers from the experts \nwill be helpful to assign numerical values for the probability \nof occurrence and the consequence of the occurrence of \neach of the identified risk drivers.  In addition to experts, we \ncan consult experienced project leaders and members of \nteams for identifying the risk factors [22].  For schedule \nmanagement, we can develop story cards to sum up the \ncustomer needs and each of these stories can be evaluated \nand estimated.  If needed, these stories can be broken into \ntasks to estimate the required effort and resources [25]. \nD. Planning Poker \nAssume that we need to estimate for project activities \nfor an Extreme programming software development method.  \nExtreme programming is perhaps the best known and most \nwidely used agile method.  The name was coined by Beck \n[3] because the approach was developed by using the best \npractices such as iterative development and customer \ninvolvement to extreme level.   Here each project activity/ \ntask that gives the client a value is called a story.  Story \nwriting is an imaginatively created account of possible \nevents those can be visualized to occur with finite \nprobabilities.  The experts with their academic background, \nprofessional experience and specialized knowledge of past, \npresent and trends for the future, synthesize situations that \nappear within the bounds of possibility and attempt to assign \nrelative probabilities to the corresponding possibilities.  \nSuch technique is extensively used by military thinkers, \nsocial planners, long-term policy-makers and the like.   \nAccurate estimates for stories or activities are crucial \nfor planning the schedule.  Based on Delphi method \nPlanning Poker was developed by Mike Cohen, the agile \nguru [18].   In this technique, each story is estimated \nindividually by a number of experts and if there is \nconsensus then the estimates are taken.  If there is a \ndisagreement, people share their thoughts and the estimation \nenters a second round. This is repeated until the estimation \nfor the story has reached a consensus.  Typically, only \nexperts participate in the Delphi Method. However, in \nsoftware project estimation, we can encourage the team \nmembers to participate.  This creates enthusiasm, ownership \nwithin the team and is a typical characteristic of Scrum \napproach.  The Scrum approach is a general agile method \nbut it focus is on managing iterative development rather \nthan specific technical approaches to agile software \nengineering.   \nX. \nPERFORMANCE OF ESTIMATION MODELS \nThere are various measures for assessing the accuracy \nof the models:  Root Mean Square Error (RMSE), Mean \nRelative Error (MRE) and so on.   RMSE is calculated \nbased on a number of actual data observed and estimated \nvalues by a model; it is derived from basic magnitude of \nrelative error which is defined as  \nN\nE\nE\nRMSE\ni\na\n/\n)\n(\n2\n\n\n\n, \n       \nwhere, Ea is the actual effort and Ei is the effort estimated by \na selected method for ith case study and N is the number of \ncase studies or number of observations.  Mean Relative \nError (MRE) is the main performance measure.  It is \nestimated from relative error, which is the relative size of \nthe difference between the actual and the estimated value.  \nMRE is the percentage of the absolute values of the relative \nerrors averaged over the number of observations.  Here \nnumber of observations is again the number of case studies \nworked out.  Hence, \ni\ni\na\nE\nE\nE\nN\nMRE\n/\n|)\n(|\n)\n/\n100\n(\n\n\n\n. \nA large positive MRE would suggest that the model \ngenerally overshoots the effort, while a large negative value \nwould indicate undershoots.  AMN Yogi and Mala V Patil \n[20, 21] have used RMSE and MRE for comparative \nassessment of Yogi and Patil, Bailey Bassili, COCOMO, \nDoty and Walston-Felix estimation models.  Boehm [4] \nspecifed ten factots for evaluating cost estimation models.  \nThey are Constructiveness, Definition, Detail, Ease of Use, \nFidelity, Objectivity, Parsimony, Prospectiveness, Stability, \nScope.  These factors give us a basic idea about the \nperformance evaluation of estimation models. \nXI.  APPLICATIONS OF DELPHI TO SOFTWARE \nRELATED STUDIES \nThere are many applications of Delphi method for \nengineering projects.  The very first application of the \nDelphi method carried out at RAND Corporation as \nillustrated in publication by Gordon and Helmer [12] was to \nassess the direction of long range trends, with special \nemphasis on science and technology and probable effects on \nsociety.  Following are the few case studies in which the \nDelphi technique was applied to software related studies: \n1. Abts and Boehm used Delphi technique to estimate \ninitial parameter values for Effort Adjustment Factor \n(EAF) appearing in glue code effort estimation \ncomponent of the COCOTS (COnstructive COTS) \nintegration Cost model [1].  EAFs are similar to the \nestimation dimensions and corresponding project \nfactors [7] which form the basis of the five sub-models \nthat comprise ESTIMACS a software estimation model \noriginally developed by Rubin [23].  Soliciting the \nopinions \nof \na \ngroup \nof \nexperienced \nsoftware \ndevelopment professionals, Abts and Boehm arrived at \nconverged values. \n2. Chulani and Boehm used the Delphi technique to \nestimate software defect introduction and removal rates \nduring various phases of software development life-\ncycle.  These factors appear in COQUALMO \n(COnstructive QUALity Model) which predicts the \nresidual defect density in terms of number of \ndefects/unit size of code [9].   \n3. Chulani and Boehm also used the Delphi technique to \nspecify the prior information required for the Bayesian \ncalibration of COCOMO II [10].  \n4. Performance evaluation of estimation models is very \nuseful for comparative assessment of these models.  It \ngives statistical insight to the accuracy of models.  To \nInternational Journal of Engineering Research & Technology (IJERT)\nISSN: 2278-0181\nPublished by, www.ijert.org\nICESMART-2015 Conference Proceedings\nVolume 3, Issue 19\nSpecial Issue - 2015\n9\n\n\nmy understanding it is very rare that expert opinion is \nsought for performance evaluation of estimation \nmodels.  However, Vicinanza, Mukhyopadhya and \nPrietula [26] used experts opinion to estimate effort \nusing Kemerer’s data set without formal algorithmic \ntechniques and found that the results outperformed the \nmodels in original study.  MRE varied from 32 to \n1107%.  Kemerer himself performed empirical \nvalidation of four algorithmic models COCOMO, \nEstimacs, Function Point Analysis (FPA) and SLIM \n[17].    \nXI. \nCONCLUSIONS \n    There are a number of factors that influence the \nsoftware development process.  The technologies and \nenvironments are changing continuously.  Most of the \nfactors representing technology and environment are \nqualitative in nature.  Quantification of these factors is \nessentially made on the basis of experience and expertise.  \nHere judgment plays a very crucial role.  An estimation \nmethod which is either non-algorithmic or algorithmic has \nto relay on quantities that are quantified by judgment.  \nKeeping the importance of judgment to quantify various \nfactors for software effort estimation in mind, in this paper I \nhave discussed about a few estimation techniques and most \nimportant the Work Breakdown Structure, the Expert \nJudgment and the Delphi Technique.  I have brought clearly \nhow Work Breakdown Structure helps in software \nestimation to find the cost of micro level components of a \nsoftware project.  WBS also becomes a tool for validating \nthe values obtained by Analogy and other methods.   \n    There are lot of differences in the environments and \nthe technologies under which the past software projects \nwere developed and the projects which are under \ndevelopment or going to be undertaken.  Only experienced \npersons will be able to forecast the future scenarios.  In the \nabsence of quantified, empirical data, expert judgment \nmethods are very useful but are highly subjective.  Success \ndepends wholly on the skills and competence of the experts \nchosen.  Problems with expert judgment methods are the \nexpertise-calibration and scalability problems for extensive \nanalyses.  Therefore, I have carefully outlined various steps \nto be followed for successful utilization of Delphi method \nfor software effort estimation. \n    As I have mentioned that every software effort \nestimation method uses expert’s knowledge in quantifying \nat least a few of its parameters.  Therefore judgment plays a \nvery important role in effort estimation.  Never the less it \nshould be noted that all software estimation techniques are \nchallenged by rapid pace of change in software technology.  \nAs per Boehm no one method should be preferred over all \nothers.  The key in arriving at sound estimates is to use \nvariety of methods and then investigate the reasons why the \nestimates provided by one might differ significantly from \nthose provided by others.  If the software engineer can \nexplain such differences to a reasonable level of satisfaction \nthen it is likely that he or she has good grasp of the factors \nwhich are driving the costs of the project at hand; and will \nbe better equipped to support the necessary project planning \nand control functions performed by management.  \n    Work Breakdown Structures, identifying risks and \nopportunities, compiling the lessons learnt, brainstorming \nsessions will help in convergence of answers.  Predicting the \nfuture is not an exact science, but the Delphi method can \nhelp us to understand the likelihood of future events and \nwhat impact they may have on our project. \nACKNOWLEDGMENTS \n    The author wishes to express his gratitude and \nheartfelt thanks to Dr. Thomus P John, Chairman, T.John \nGroup of Institutions and Dr. Thippeswamy HN, Principal, \nT.John Institute of Technology, Bengaluru for their support \nand encouragement during preparation of this paper.  \nREFERENCES \n[1] \nAbts,C, Baily, B and Boehm B.  “COCOTOS Software Integration \nCost Model: An Overview”. Proceedings of the California Software \nSymposium,  1998. \n[2] \nBaird B. Managerial Decisions Under Uncertainty, John Wiley & \nSons, 1989. \n[3] \nBeck, K. Extreme Programming Explained. Addition- Wesley, 2000. \n[4] \nBoehm, B.  Software Engineering Economics.  Eaglewood Cliffs, NJ: \nPrentice Hall. 1981. \n[5] \nBoehm, B. W. and Royce, W.   Ada COCOMO and the Ada process \nmodel.  Proc. 5th COCOMO users’ Group Meeting, Pittsburgh: \nSoftware Engineering Institute. 1989. \n[6] \nBoehm B. W, Chris Abts and Sunita Chulani, “Software development \ncost estimation Approaches – A survey”, Annals of Software \nEngineering, Vol. 10, No.1-4, pp. 177-205, 2000. \n[7] \nBoehm, B. W. Abts, C. et al. Software Cost Estimation with \nCOCOMO II.  Upper Saddle River. NJ: Prentice Hall. 2000. \n[8] \nBoehm B. W and Ellis Harrowitz, “Software Cost Estimation with \nCOCOMO II”, Prentice Hall, 2000. \n[9] \nChulani S. “Modeling Defect Introduction”, California Software \nSymposium – Nov. 1997. \n[10] Chulani S, Boehm B and Steece B. (1998). “Calibrating Software \nCost Models using Bayesian Analysis.  IEEE Transaction on \nSoftware Engineering.  Special Issue on Empirical methods in \nSoftware Engineering. 1998. \n[11] Fred Heemstra, F. J.  ‘Software Cost Estimation’, Information & \nSoftware Technol 34 (10) pp 627-639. 1992. \n[12] Gordon, T. J., and Olaf Helmer. Report on a Long-Range Forecasting \nStudy. P-2982 . Santa Monica (CA): The RAND Corporation. 1964 \n[13] Helmer Olaf and Rescher Nicholas. On Epistemology of inexact \nSciences, Mgt Science, Vol. 6, pp. 47. 1959 \n[14] Helmer, O. Social Technology,  NY. 1966. \n[15] Helmer, O. Reassessment of cross-impact analysis. Futures, pp. 389-\n400. Oct. 1981 \n[16] Hughes, R.T., ‘Expert Judgment as the estimation method’. Inf. and \nSoftware Technol. 38(2) pp. 67-75, 1996. \n[17] Kermerer, C. F. “An empirical validation of software cost estimation \nmodels:. Communications of the ACM Vol. 30, No. 5 1987, pp-416-\n429. \n[18]  Mike Cohen “Agile Estimating and Planning”.  Prentice Hall Nov. \n2005. \n[19] Nageswara Yogi AM.  “A model for LCC Estimation for Defence \nEquipment”, Proc. of Int. Conf. on Trends in PLMSS-2006, pp. 415-\n423. 2006. \n[20] Nageswara Yogi AM and Mala. V. Patil.  Software effort estimation \nmodels and performance analysis with case studies.”  Int. J. of Comp. \nApp. in Engineering, Technology and Sciences. Vol. 1 No. 2, 2009. \nPp 558-565. \n[21] Mala. V. Patil and Nageswara Yogi AM. “Importance of data \ncollection and validation of for Systematic Software Development \nProcess”. Int. J. of Comp. Sc. & Inf. Tech.  Vol. 3 No. 2. 2011.   \nInternational Journal of Engineering Research & Technology (IJERT)\nISSN: 2278-0181\nPublished by, www.ijert.org\nICESMART-2015 Conference Proceedings\nVolume 3, Issue 19\nSpecial Issue - 2015\n10\n\n\n[22] Nageswara Yogi AM and Mala. V. Patil.  “Identifying Risks and \nPossible Remedies to Mitigate Them during Systematic Software \nDevelopment Process”.  Int. J. of Engg. Res. and Tech. NCSE’14 \nConference Proceedings Feb. 2014. \n[23] Rubin H, “ESTIMATICS”, IEEE, 1983. \n[24] Steven Fraser, Boehm B. W. Hakan Erdogmus. Jørgensen M. Stan \nRifkin. Mike Ross, “The Role of Judgment in Software Estimation,” \nICSE’ 09, pp. 13-17, 2009. \n[25] Somerville, Software Engineering 2007 \n[26] Vicnanza S. S., Mukhyopadhya, T and Prietula, M. J. “Software-\neffort estimation: an exploratory study of expert performance”, \nInformation Systems Research, Vol. 2 no.4 1991, pp-243-262. \n[27] Vigder, M. R., and A. W Kark. “Software cost estimation and control. \nFeb. 1999, Report available in the net. \n[28] http://www.softwaretestingclass.com/software-estimation-techniques/ \n[29] http://www.unf.edu/~ncoulter/cen6070/ handouts / ManagingRisk.pdf \n[30]http://www.developer.com/java/other/article.php/1463281[31]\n \nhttp://yunus.hacettepe.edu.tr/~sencer/cocomo.html/ \n[32] http://www.brighthubpm.com/project planning/121839 \n[33] http://www.standishgroup.com\n \nInternational Journal of Engineering Research & Technology (IJERT)\nISSN: 2278-0181\nPublished by, www.ijert.org\nICESMART-2015 Conference Proceedings\nVolume 3, Issue 19\nSpecial Issue - 2015\n11\n",
  "metadata": {
    "filename": "delphi-technique-for-the-software-effort-estimation-an-outline-for-an-expert-judgment-method-IJERTCONV3IS19088.pdf",
    "num_pages": 11,
    "title": "Delphi Technique for the Software Effort Estimation -- An Outline for an Expert Judgment Method",
    "author": "Dr. A M Nageswara Yogi,",
    "subject": "IJERT.COM - International Journal of Engineering Research and Technology"
  },
  "num_pages": 11,
  "timestamp": "2025-11-13T02:00:03.929206"
}